{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d0613eb-dae6-4a59-a28e-373e42c8d674",
   "metadata": {},
   "source": [
    "## Adaptive Tech Support for Older Adults\n",
    "\n",
    "This is an automated system that generates step-by-step tech support guides with screenshots for older adults based on user queries viaprompt chaining. By retrieving relevant YouTube tutorials via the YouTube API, extracting key UI elements using OpenCV, and creating visual guides with the OS-ATLAS action model, the system aims to enhance the accessibility and effectiveness of tech support for older adults by leveraging video tutorial content in a more accessible, image-based format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21e24df-48b0-4321-8f97-f9f3c8f59ead",
   "metadata": {},
   "source": [
    "### 1. Installation of Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934f63cf-1850-4276-ba7c-f8c39ddf0245",
   "metadata": {},
   "source": [
    "This cell installs the necessary Python libraries for the project, including packages for Google API access, video processing, image manipulation, machine learning, and deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77269496-4d4e-41f5-bd1c-8e31165b2db0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client\n",
    "!pip install opencv-python-headless yt-dlp\n",
    "!pip install scikit-image\n",
    "!pip install qwen_vl_utils\n",
    "!pip install transformers\n",
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401fc256-5522-4569-ac40-f9ad7b422710",
   "metadata": {},
   "source": [
    "### 2. YouTube Video Search and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae02153-5b34-4fe4-89ef-41197540faf4",
   "metadata": {},
   "source": [
    "This cell performs the following tasks:\n",
    "- **Searches for YouTube videos** based on a user-provided query using the YouTube Data API.\n",
    "- **Cleans the query** to make it folder-friendly (removes spaces and special characters).\n",
    "- **Fetches video details** including title, URL, and view count.\n",
    "- **Selects the video with the highest number of views** from the search results.\n",
    "- The user is prompted to enter a search query, and the most popular video is displayed with its title, URL, and view count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f28b3e-9062-4110-842a-80687c621c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "# YouTube API key\n",
    "YOUTUBE_API_KEY = \"\" # Add your API Key here\n",
    "\n",
    "def clean_query(query):\n",
    "    \"\"\"Convert query to a folder-friendly format (remove spaces & special characters).\"\"\"\n",
    "    return re.sub(r'\\W+', '_', query.strip())\n",
    "\n",
    "def search_youtube_video(query, max_results=5):\n",
    "    search_url = \"https://www.googleapis.com/youtube/v3/search\"\n",
    "    search_params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"q\": query,\n",
    "        \"type\": \"video\",\n",
    "        \"maxResults\": max_results,\n",
    "        \"key\": YOUTUBE_API_KEY\n",
    "    }\n",
    "    response = requests.get(search_url, params=search_params).json()\n",
    "\n",
    "    # Extract video IDs\n",
    "    video_results = []\n",
    "    video_ids = []\n",
    "    for item in response.get(\"items\", []):\n",
    "        video_id = item[\"id\"][\"videoId\"]\n",
    "        title = item[\"snippet\"][\"title\"]\n",
    "        video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "        video_ids.append(video_id)\n",
    "        video_results.append({\"id\": video_id, \"title\": title, \"url\": video_url})\n",
    "    if not video_results:\n",
    "        return None\n",
    "    \n",
    "    # Fetch video statistics (views) using a separate API call\n",
    "    stats_url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "    stats_params = {\n",
    "        \"part\": \"statistics\",\n",
    "        \"id\": \",\".join(video_ids),\n",
    "        \"key\": YOUTUBE_API_KEY\n",
    "    }\n",
    "    stats_response = requests.get(stats_url, params=stats_params).json()\n",
    "    # Map video views to results\n",
    "    for i, item in enumerate(stats_response.get(\"items\", [])):\n",
    "        video_results[i][\"views\"] = int(item[\"statistics\"].get(\"viewCount\", 0))\n",
    "    # Select the video with the highest views\n",
    "    best_video = max(video_results, key=lambda x: x[\"views\"])\n",
    "    return best_video\n",
    "\n",
    "# Dynamic user input\n",
    "query = input(\"Enter your search query: \")\n",
    "query_folder = clean_query(query)  # Ensure query is folder-safe\n",
    "best_video = search_youtube_video(query)\n",
    "\n",
    "if best_video:\n",
    "    print(f\"Selected Video: {best_video['title']} - {best_video['url']} ({best_video['views']} views)\")\n",
    "else:\n",
    "    print(\"No videos found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e21f38f-bf39-4402-86b0-cd236d292595",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3. Video Download and Folder Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79184554-0357-4b95-bef3-ca557a4294a6",
   "metadata": {},
   "source": [
    "This cell performs the following tasks:\n",
    "- **Clears cached data** from yt-dlp to ensure fresh downloads.\n",
    "- **Sets up folders** for storing the video and screenshots related to the search query:\n",
    "  - Deletes any existing folders and creates new ones.\n",
    "- **Downloads the selected YouTube video**:\n",
    "  - The video is saved in the newly created folder for the search query.\n",
    "  - The `yt-dlp` tool is configured to download the best available quality.\n",
    "  - A cookie file (if available) is used for the download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca0d113-33fa-4794-ae10-8118baed4e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "yt_dlp.YoutubeDL().cache.remove()\n",
    "# if os.path.exists('cookies.txt'):\n",
    "#     os.remove('cookies.txt')\n",
    "\n",
    "def setup_folders(query_folder):\n",
    "    \"\"\"Delete old data & create new folders for the query.\"\"\"\n",
    "    base_dirs = [\"videos\", \"screenshots\"]\n",
    "    for base in base_dirs:\n",
    "        folder_path = os.path.join(base, query_folder)\n",
    "        if os.path.exists(folder_path):\n",
    "            shutil.rmtree(folder_path)  # Delete old folder\n",
    "        os.makedirs(folder_path)  # Create fresh folder\n",
    "\n",
    "def download_video(video_url, output_folder=\"videos\"):\n",
    "    query_video_folder = os.path.join(output_folder, query_folder)\n",
    "    os.makedirs(query_video_folder, exist_ok=True)  # Ensure folder exists\n",
    "    ydl_opts = {\n",
    "        \"format\": \"best\",\n",
    "        \"outtmpl\": f\"{query_video_folder}/%(id)s.%(ext)s\",\n",
    "        \"cookiefile\": \"cookies.txt\",\n",
    "        \"force-ipv4\": True,\n",
    "        # \"compat_opts\": [\"legacy-server\"]\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.cache.remove()\n",
    "        ydl.download([video_url])\n",
    "\n",
    "# Setup folders for new query\n",
    "setup_folders(query_folder)\n",
    "# Use the video with the highest views\n",
    "download_video(best_video[\"url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a554ad21-5cb3-42ec-a581-eca9e9321f4d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4. Phone Screen Detection and Screenshot Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c738cf46-9e57-43f1-a1f7-deac7a324d4e",
   "metadata": {},
   "source": [
    "This cell performs the following tasks:\n",
    "- **Detects the phone screen** in video frames using edge detection and contour approximation:\n",
    "  - Converts frames to grayscale and uses Canny edge detection.\n",
    "  - Finds the largest contour, approximates it to a polygon, and identifies it as the phone screen if it is a quadrilateral.\n",
    "- **Extracts relevant screenshots** from the video:\n",
    "  - Screenshots are taken at intervals, and only frames where the phone screen is detected are processed.\n",
    "  - Uses Structural Similarity Index (SSIM) to filter out similar frames, saving only frames with noticeable UI changes.\n",
    "  - Screenshots are saved in the specified folder for the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b282303-addd-4c14-a2d6-82b9357a398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def detect_phone_screen(frame):\n",
    "    \"\"\"Detects the largest rectangle (phone screen) in the frame using edge detection and contour approximation.\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return None  # No contours found\n",
    "    # Find the largest rectangle\n",
    "    max_contour = max(contours, key=cv2.contourArea)\n",
    "    epsilon = 0.02 * cv2.arcLength(max_contour, True)\n",
    "    approx = cv2.approxPolyDP(max_contour, epsilon, True)\n",
    "    if len(approx) == 4:  # If it's a quadrilateral, assume it's the phone screen\n",
    "        return approx.reshape(4, 2)\n",
    "    return None\n",
    "\n",
    "def extract_relevant_frames(video_path, output_folder=\"screenshots\", frame_interval=1, similarity_threshold=0.98):\n",
    "    query_screenshot_folder = os.path.join(output_folder, query_folder)\n",
    "    os.makedirs(query_screenshot_folder, exist_ok=True)  # Ensure folder exists\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    screenshot_count = 0\n",
    "    last_saved_frame = None\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Skip frames to reduce processing time\n",
    "        if frame_count % frame_interval == 0:\n",
    "            phone_screen = detect_phone_screen(frame)\n",
    "            if phone_screen is not None:  # Only process frames where the phone screen is detected\n",
    "                if last_saved_frame is None:\n",
    "                    save_path = f\"{query_screenshot_folder}/frame{screenshot_count}.jpg\"\n",
    "                    cv2.imwrite(save_path, frame)\n",
    "                    last_saved_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                    screenshot_count += 1\n",
    "                else:\n",
    "                    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                    similarity = ssim(last_saved_frame, gray_frame)\n",
    "                    if similarity < similarity_threshold:  # Only save if UI has changed\n",
    "                        save_path = f\"{query_screenshot_folder}/frame_{screenshot_count}.jpg\"\n",
    "                        cv2.imwrite(save_path, frame)\n",
    "                        last_saved_frame = gray_frame\n",
    "                        screenshot_count += 1\n",
    "        frame_count += 1\n",
    "    cap.release()\n",
    "    print(f\"Extracted {screenshot_count} relevant UI screenshots in '{query_screenshot_folder}'.\")\n",
    "# Run extraction with filtering\n",
    "video_file = f\"videos/{query_folder}/{best_video['id']}.mp4\"\n",
    "extract_relevant_frames(video_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46b4bf7-b312-4eda-b62e-32ce7cf420e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 5. Phone Screen Detection and Cropping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054fa68d-65fd-4c4a-92a8-eaaa38e6a525",
   "metadata": {},
   "source": [
    "This cell contains functions for detecting and extracting the screen region from images, using methods like adaptive thresholding, Canny edge detection, and contour analysis. It includes:\n",
    "- `detect_phone_screen(image)`: Detects the phone's screen area by analyzing edges, contours, and aspect ratios.\n",
    "- `crop_screen_with_perspective(image, screen_contour)`: Crops the detected screen area using a perspective transformation.\n",
    "- `order_points(pts)`: Orders the contour points to ensure correct perspective transformation.\n",
    "- `visualize_detection(image, screen_contour, filename=None, output_dir=None)`: Visualizes the detected screen contour for debugging.\n",
    "- `extract_ui_screenshots(input_folder=\"screenshots\", output_folder=\"ui-screens\")`: Extracts and saves only the phone screen regions from screenshots.\n",
    "\n",
    "These functions help isolate the phone screen from images and ensure correct transformations for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88b613e-e25e-4f59-8a91-2c5a2ce6b590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def detect_phone_screen(image):\n",
    "    \"\"\"Detects the phone in the image, focusing on the likely phone region.\"\"\"\n",
    "    # Create a copy of the image\n",
    "    original = image.copy()\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # First, try to use the fact that phones often have bright screens\n",
    "    # that stand out from the background\n",
    "    \n",
    "    # 1. Try adaptive thresholding to find the screen\n",
    "    thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                  cv2.THRESH_BINARY_INV, 11, 2)\n",
    "    \n",
    "    # 2. Also try Canny edge detection\n",
    "    canny = cv2.Canny(blurred, 50, 150)\n",
    "    \n",
    "    # Combine both methods\n",
    "    edges = cv2.bitwise_or(thresh, canny)\n",
    "    \n",
    "    # Dilate to connect edges\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    dilated = cv2.dilate(edges, kernel, iterations=1)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(dilated.copy(), cv2.RETR_EXTERNAL, \n",
    "                                  cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Sort contours by area in descending order\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "    \n",
    "    # Additional filter for phone-like aspect ratio\n",
    "    phone_contours = []\n",
    "    \n",
    "    for contour in contours[:10]:  # Only check the 10 largest contours\n",
    "        # Approximate the contour\n",
    "        peri = cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, 0.02 * peri, True)\n",
    "        \n",
    "        # Get bounding rectangle\n",
    "        x, y, w, h = cv2.boundingRect(approx)\n",
    "        \n",
    "        # Skip if the contour is too small\n",
    "        if w * h < (width * height) / 20:\n",
    "            continue\n",
    "            \n",
    "        # Check if it's at least somewhat rectangular (4-ish points)\n",
    "        if len(approx) >= 4 and len(approx) <= 8:\n",
    "            # Check if it has phone-like aspect ratio (height > width)\n",
    "            aspect_ratio = h / w if w > 0 else 0\n",
    "            \n",
    "            # Phones are typically taller than they are wide (aspect ratio > 1.5)\n",
    "            if aspect_ratio > 1.5 and aspect_ratio < 2.5:\n",
    "                phone_contours.append(approx)\n",
    "    \n",
    "    # If we found potential phone contours, use the largest one\n",
    "    if phone_contours:\n",
    "        largest_phone_contour = max(phone_contours, key=cv2.contourArea)\n",
    "        # Get the bounding rectangle\n",
    "        x, y, w, h = cv2.boundingRect(largest_phone_contour)\n",
    "        \n",
    "        # Create a 4-point contour from the rectangle for perspective transform\n",
    "        screen_contour = np.array([\n",
    "            [x, y],           # Top-left\n",
    "            [x + w, y],       # Top-right\n",
    "            [x + w, y + h],   # Bottom-right\n",
    "            [x, y + h]        # Bottom-left\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        return screen_contour\n",
    "    \n",
    "    # If no phone contour found, look for a rectangle with phone-like proportions\n",
    "    # in the center half of the image (where phones usually are in tutorial videos)\n",
    "    \n",
    "    # Define the center region (middle 50% of the image)\n",
    "    center_x = width // 4\n",
    "    center_y = height // 4\n",
    "    center_w = width // 2\n",
    "    center_h = height // 2\n",
    "    \n",
    "    # Create a mask for the center region\n",
    "    mask = np.zeros_like(gray)\n",
    "    mask[center_y:center_y+center_h, center_x:center_x+center_w] = 255\n",
    "    \n",
    "    # Apply the mask to the edge image\n",
    "    masked_edges = cv2.bitwise_and(dilated, mask)\n",
    "    \n",
    "    # Find contours in the center region\n",
    "    center_contours, _ = cv2.findContours(masked_edges, cv2.RETR_EXTERNAL, \n",
    "                                         cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Sort by area\n",
    "    center_contours = sorted(center_contours, key=cv2.contourArea, reverse=True)\n",
    "    \n",
    "    for contour in center_contours[:5]:\n",
    "        # Approximate the contour\n",
    "        peri = cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, 0.02 * peri, True)\n",
    "        \n",
    "        # Get bounding rectangle\n",
    "        x, y, w, h = cv2.boundingRect(approx)\n",
    "        \n",
    "        # Skip small contours\n",
    "        if w * h < (width * height) / 30:\n",
    "            continue\n",
    "            \n",
    "        # Check aspect ratio (phone-like)\n",
    "        aspect_ratio = h / w if w > 0 else 0\n",
    "        if aspect_ratio > 1.5 and aspect_ratio < 2.5:\n",
    "            # Create a 4-point contour\n",
    "            screen_contour = np.array([\n",
    "                [x, y],           # Top-left\n",
    "                [x + w, y],       # Top-right\n",
    "                [x + w, y + h],   # Bottom-right\n",
    "                [x, y + h]        # Bottom-left\n",
    "            ], dtype=np.float32)\n",
    "            \n",
    "            return screen_contour\n",
    "    \n",
    "    # If we still haven't found anything, try one more approach:\n",
    "    # Find the most central large rectangle in the image\n",
    "    \n",
    "    # Use Hough Line transform to find straight lines\n",
    "    lines = cv2.HoughLinesP(edges, 1, np.pi/180, 100, minLineLength=100, maxLineGap=10)\n",
    "    \n",
    "    if lines is not None:\n",
    "        # Draw the lines on a blank image\n",
    "        line_image = np.zeros_like(gray)\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            cv2.line(line_image, (x1, y1), (x2, y2), 255, 2)\n",
    "        \n",
    "        # Find contours in the line image\n",
    "        line_contours, _ = cv2.findContours(line_image, cv2.RETR_EXTERNAL, \n",
    "                                           cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # Find the contour closest to the center of the image\n",
    "        image_center = np.array([width/2, height/2])\n",
    "        best_distance = float('inf')\n",
    "        best_contour = None\n",
    "        \n",
    "        for contour in line_contours:\n",
    "            if cv2.contourArea(contour) < (width * height) / 20:\n",
    "                continue\n",
    "                \n",
    "            # Find center of contour\n",
    "            M = cv2.moments(contour)\n",
    "            if M[\"m00\"] != 0:\n",
    "                cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "                contour_center = np.array([cx, cy])\n",
    "                \n",
    "                # Calculate distance to image center\n",
    "                distance = np.linalg.norm(contour_center - image_center)\n",
    "                \n",
    "                # Get bounding rect to check aspect ratio\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                aspect_ratio = h / w if w > 0 else 0\n",
    "                \n",
    "                # If it's phone-like and closer to center than previous best\n",
    "                if aspect_ratio > 1.5 and aspect_ratio < 2.5 and distance < best_distance:\n",
    "                    best_distance = distance\n",
    "                    best_contour = contour\n",
    "        \n",
    "        if best_contour is not None:\n",
    "            # Get the bounding rectangle\n",
    "            x, y, w, h = cv2.boundingRect(best_contour)\n",
    "            \n",
    "            # Create a 4-point contour\n",
    "            screen_contour = np.array([\n",
    "                [x, y],           # Top-left\n",
    "                [x + w, y],       # Top-right\n",
    "                [x + w, y + h],   # Bottom-right\n",
    "                [x, y + h]        # Bottom-left\n",
    "            ], dtype=np.float32)\n",
    "            \n",
    "            return screen_contour\n",
    "    \n",
    "    # If all else fails, just take the center portion of the image \n",
    "    # with a typical phone aspect ratio (this is a fallback)\n",
    "    center_x = width // 3\n",
    "    center_width = width // 3\n",
    "    \n",
    "    # Calculate height based on typical phone aspect ratio (1:2)\n",
    "    center_height = center_width * 2\n",
    "    center_y = (height - center_height) // 2\n",
    "    \n",
    "    # Create a rectangle\n",
    "    screen_contour = np.array([\n",
    "        [center_x, center_y],                       # Top-left\n",
    "        [center_x + center_width, center_y],        # Top-right\n",
    "        [center_x + center_width, center_y + center_height],  # Bottom-right\n",
    "        [center_x, center_y + center_height]        # Bottom-left\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    return screen_contour\n",
    "\n",
    "# def crop_screen(image, screen_contour):\n",
    "#     \"\"\"Simply crops the region inside the screen contour (no perspective correction).\"\"\"\n",
    "#     if screen_contour is None or len(screen_contour) != 4:\n",
    "#         return None\n",
    "    \n",
    "#     # Get the bounding rectangle for the contour\n",
    "#     x, y, w, h = cv2.boundingRect(screen_contour)\n",
    "    \n",
    "#     # Crop the image using the bounding rectangle\n",
    "#     cropped = image[y:y+h, x:x+w]\n",
    "    \n",
    "#     return cropped\n",
    "\n",
    "def crop_screen_with_perspective(image, screen_contour):\n",
    "    \"\"\"Crops the region inside the screen contour using a perspective transform.\"\"\"\n",
    "    if screen_contour is None or len(screen_contour) != 4:\n",
    "        return None\n",
    "\n",
    "    # Order points in a consistent order (top-left, top-right, bottom-right, bottom-left)\n",
    "    ordered_points = order_points(screen_contour)\n",
    "    \n",
    "    # Define the dimensions for the perspective transform\n",
    "    width = 360  # Desired width of the output screen crop\n",
    "    height = 640  # Desired height of the output screen crop\n",
    "    dst_points = np.array([\n",
    "        [0, 0],\n",
    "        [width - 1, 0],\n",
    "        [width - 1, height - 1],\n",
    "        [0, height - 1]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # Get the perspective transform matrix\n",
    "    matrix = cv2.getPerspectiveTransform(ordered_points, dst_points)\n",
    "\n",
    "    # Apply perspective warp\n",
    "    warped = cv2.warpPerspective(image, matrix, (width, height))\n",
    "    return warped\n",
    "\n",
    "def order_points(pts):\n",
    "    \"\"\"Orders points in top-left, top-right, bottom-right, bottom-left order.\"\"\"\n",
    "    # Sort by y-coordinate (top to bottom)\n",
    "    sorted_by_y = pts[np.argsort(pts[:, 1])]\n",
    "    \n",
    "    # Get top and bottom points\n",
    "    top_points = sorted_by_y[:2]\n",
    "    bottom_points = sorted_by_y[2:]\n",
    "    \n",
    "    # Sort top points by x-coordinate (left to right)\n",
    "    top_left, top_right = top_points[np.argsort(top_points[:, 0])]\n",
    "    \n",
    "    # Sort bottom points by x-coordinate (left to right)\n",
    "    bottom_left, bottom_right = bottom_points[np.argsort(bottom_points[:, 0])]\n",
    "    \n",
    "    # Return points in order: top-left, top-right, bottom-right, bottom-left\n",
    "    return np.array([top_left, top_right, bottom_right, bottom_left], dtype=np.float32)\n",
    "\n",
    "def visualize_detection(image, screen_contour, filename=None, output_dir=None):\n",
    "    \"\"\"Visualizes the detected screen contour for debugging.\"\"\"\n",
    "    vis = image.copy()\n",
    "    \n",
    "    if screen_contour is not None:\n",
    "        # Draw the contour\n",
    "        cv2.drawContours(vis, [screen_contour.astype(int)], -1, (0, 255, 0), 2)\n",
    "        \n",
    "        # Draw the ordered corners\n",
    "        rect = order_points(screen_contour)\n",
    "        colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0)]  # BGR for corners\n",
    "        labels = [\"TL\", \"TR\", \"BR\", \"BL\"]  # Corner labels\n",
    "        \n",
    "        for i, (x, y) in enumerate(rect.astype(int)):\n",
    "            cv2.circle(vis, (x, y), 8, colors[i], -1)\n",
    "            cv2.putText(vis, labels[i], (x - 10, y - 10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, colors[i], 2)\n",
    "    \n",
    "    if filename and output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        save_path = os.path.join(output_dir, f\"debug_{filename}\")\n",
    "        cv2.imwrite(save_path, vis)\n",
    "    \n",
    "    return vis\n",
    "\n",
    "def extract_ui_screenshots(input_folder=\"screenshots\", output_folder=\"ui-screens\"):\n",
    "    \"\"\"Extracts only the screen region from screenshots and saves the raw cropped area.\"\"\"\n",
    "    query_ui_folder = os.path.join(output_folder, query_folder)\n",
    "    debug_folder = os.path.join(output_folder, \"debug\")\n",
    "    \n",
    "    os.makedirs(query_ui_folder, exist_ok=True)\n",
    "    os.makedirs(debug_folder, exist_ok=True)\n",
    "    \n",
    "    screenshot_files = sorted(os.listdir(os.path.join(input_folder, query_folder)))\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for screenshot in screenshot_files:\n",
    "        img_path = os.path.join(input_folder, query_folder, screenshot)\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        if img is None:\n",
    "            # print(f\"Could not read image: {img_path}\")\n",
    "            failed += 1\n",
    "            continue\n",
    "        \n",
    "        # Detect phone screen\n",
    "        screen_contour = detect_phone_screen(img)\n",
    "        \n",
    "        # Create visualization for debugging\n",
    "        visualize_detection(img, screen_contour, screenshot, debug_folder)\n",
    "        \n",
    "        if screen_contour is not None:\n",
    "            # Simply crop the region inside the green box without perspective correction\n",
    "            cropped_screen = crop_screen_with_perspective(img, screen_contour)\n",
    "            \n",
    "            if cropped_screen is not None and cropped_screen.size > 0:\n",
    "                save_path = os.path.join(query_ui_folder, screenshot)\n",
    "                cv2.imwrite(save_path, cropped_screen)\n",
    "                # print(f\"Successfully processed: {screenshot}\")\n",
    "                successful += 1\n",
    "            else:\n",
    "                # print(f\"Failed to process: {screenshot} - Invalid crop\")\n",
    "                failed += 1\n",
    "        else:\n",
    "            print(f\"âœ— Failed to detect screen in: {screenshot}\")\n",
    "            failed += 1\n",
    "    \n",
    "    print(f\"Extraction complete!\")\n",
    "    print(f\"Successfully processed: {successful} images\")\n",
    "    print(f\"Failed to process: {failed} images\")\n",
    "    print(f\"Extracted UI screens saved in '{query_ui_folder}'\")\n",
    "    print(f\"Debug visualizations saved in '{debug_folder}'\")\n",
    "\n",
    "# Run UI screen extraction\n",
    "extract_ui_screenshots()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9528398a-30d6-48de-946f-ce380081cd6f",
   "metadata": {},
   "source": [
    "### 6. OS-Atlas Action Model for UI Bounding Box Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3ca2a2-9dda-49ed-b540-e34f1870fbf6",
   "metadata": {},
   "source": [
    "This cell processes UI screenshots to generate step-by-step instructions for a user query, such as a technical support task. It processes a series of UI screenshots, performs action history tracking, and generates step-by-step instructions with bounding boxes for UI elements.\n",
    "\n",
    "- Bounding box generation\n",
    "- Interaction keyword detection (e.g., button, text, icon, etc.)\n",
    "- Step-by-step instructions generation for UI actions\n",
    "- Action history tracking\n",
    "- GPU memory management\n",
    "- Uses the `Qwen2VLForConditionalGeneration` model from Hugging Face\n",
    "\n",
    "This script is used for extracting detailed UI instructions, enhancing bounding boxes, and providing a more refined view of the UI interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60684a94-0eb9-4c15-99c3-36da09687b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "import gc\n",
    "\n",
    "# Clean up GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "# Define interaction keywords (list of terms related to UI actions)\n",
    "interaction_keywords = ['button', 'text', 'icon', 'menu', 'checkbox', 'field', 'label']\n",
    "\n",
    "def os_atlas_enhanced_bounding_boxes(query_folder, user_query):\n",
    "    \"\"\"\n",
    "    Enhanced version of OS-Atlas with more precise bounding box generation and action history tracking.\n",
    "    \n",
    "    Args:\n",
    "        query_folder: Folder name inside the screenshots directory containing UI images\n",
    "        user_query: The specific technical support task to analyze\n",
    "    \"\"\"\n",
    "    # Initialize action history\n",
    "    action_history = []\n",
    "\n",
    "    # Setup paths\n",
    "    ui_screens_folder = f\"ui-screens/{query_folder}\"\n",
    "    output_folder = f\"results/{query_folder}\"\n",
    "    bbox_folder = os.path.join(output_folder, \"bounding_boxes\")\n",
    "    \n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    os.makedirs(bbox_folder, exist_ok=True)\n",
    "    \n",
    "    # Load model and processor\n",
    "    print(\"Loading model...\")\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        \"OS-Copilot/OS-Atlas-Pro-7B\", \n",
    "        torch_dtype=torch.float16\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(\n",
    "        \"OS-Copilot/OS-Atlas-Pro-7B\",\n",
    "        size={\"shortest_edge\": 224, \"longest_edge\": 224}\n",
    "    )\n",
    "    \n",
    "    # Load the UI screenshots\n",
    "    print(f\"Loading screenshots from {ui_screens_folder}...\")\n",
    "    frame_files = sorted(\n",
    "        [f for f in os.listdir(ui_screens_folder) if f.startswith(\"frame_\")],\n",
    "        key=lambda x: int(x.split('_')[1].split('.')[0])\n",
    "    )\n",
    "    \n",
    "    if not frame_files:\n",
    "        print(f\"No frames found in {ui_screens_folder}\")\n",
    "        return\n",
    "    \n",
    "    # Prepare a more focused system prompt    \n",
    "    sys_prompt = f\"\"\"\n",
    "    You are operating in Executable Language Grounding mode as an expert mobile UI assistant.\n",
    "    Your task is to break down the query \"{user_query}\" into clear, executable steps tied to specific UI elements.\n",
    "    \n",
    "    Please generate at least 5 clear steps for the user query {user_query} with detailed UI actions in the format:\n",
    "    1. **CLICK [button name]**: Click on the button to perform an action.\n",
    "    2. **TYPE [text input]**: Type the provided text in the field.\n",
    "    3. **SCROLL [direction]**: Scroll in the specified direction.\n",
    "    4. **WAIT**: Wait for the UI to respond.\n",
    "    5. **OPEN_APP [app_name]**: Open the specified app.\n",
    "\n",
    "    Do not leave the list incomplete.\n",
    "\n",
    "    Ensure that each action is specific and describes the UI element involved.\n",
    "    In most cases, task instructions are high-level and abstract. Carefully read the instruction and action history, then perform reasoning to determine the most appropriate next action. Ensure you strictly generate two sections: Thoughts and Actions.\n",
    "    \n",
    "    Thoughts: Clearly outline your reasoning process for current step.\n",
    "    Actions: Specify the actual UI actions you will take based on your reasoning. You should follow action format above when generating. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Load images and keep track of paths\n",
    "    frames = []\n",
    "    frame_paths = []\n",
    "    for frame_file in frame_files:\n",
    "        frame_path = os.path.join(ui_screens_folder, frame_file)\n",
    "        frame_paths.append(frame_path)\n",
    "        image = Image.open(frame_path).convert('RGB')\n",
    "        frames.append(image)\n",
    "    \n",
    "    # Prepare the message with images\n",
    "    message_content = [\n",
    "        {\"type\": \"text\", \"text\": sys_prompt},\n",
    "        {\"type\": \"text\", \"text\": f\"Specific task: {user_query}\"}\n",
    "    ]\n",
    "    \n",
    "    # Add all frames as images\n",
    "    for image in frames:\n",
    "        message_content.append({\"type\": \"image\", \"image\": image})\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": message_content}]\n",
    "    \n",
    "    # Process with the model\n",
    "    print(\"Generating response...\")\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(text=text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move inputs to GPU\n",
    "    inputs = {k: v.to(\"cuda\") if hasattr(v, 'to') else v for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate output with more focused token generation\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=256, temperature=0.2, do_sample=True)\n",
    "    \n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids, \n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )[0]\n",
    "    \n",
    "    # Clean up the output\n",
    "    output_text = re.sub(r'system.*?user', '', output_text, flags=re.DOTALL)\n",
    "    output_text = re.sub(r'assistant', '', output_text)\n",
    "    \n",
    "    # Extract just the step-by-step instructions\n",
    "    instructions_match = re.search(r'Step 1:.*', output_text, re.DOTALL)\n",
    "    if instructions_match:\n",
    "        cleaned_instructions = instructions_match.group(0)\n",
    "    else:\n",
    "        # Fallback if no steps found\n",
    "        cleaned_instructions = re.sub(r'Task to analyze:.*?\\n', '', output_text)\n",
    "    \n",
    "    # Save the response\n",
    "    with open(os.path.join(output_folder, \"instructions.txt\"), \"w\") as f:\n",
    "        f.write(cleaned_instructions)\n",
    "    \n",
    "    # Enhanced bounding box generation\n",
    "    print(\"Creating enhanced bounding box images...\")\n",
    "    \n",
    "    # Extract step information\n",
    "    steps = re.findall(r'thoughts:\\s*(.*?)\\s*actions:\\s*(.*?)\\s*(?=(thoughts:|$))', output_text, re.DOTALL)\n",
    "    \n",
    "    # For each step, extract thoughts and actions\n",
    "    for step_num, step in enumerate(steps, 1):\n",
    "        thoughts, actions, _ = step # Unpack the thoughts, actions, and discard the last match\n",
    "        try:\n",
    "            print(f\"Step {step_num} thoughts: {thoughts}\")\n",
    "            print(f\"Step {step_num} actions: {actions}\")\n",
    "            \n",
    "            # Assuming each step has associated thoughts and actions, append them to action history\n",
    "            action_history.append(f\"Thoughts: {thoughts.strip()}\")\n",
    "            action_history.append(f\"Actions: {actions.strip()}\")\n",
    "    \n",
    "            # Extract the UI elements or actions from the step (you can further process if needed)\n",
    "            img = np.array(frames[step_num - 1])\n",
    "            ui_elements = detect_enhanced_ui_elements(img, actions, interaction_keywords)\n",
    "            if not ui_elements:\n",
    "                ui_elements = detect_ui_elements(img)\n",
    "            \n",
    "            # Create a copy of the original image for bounding box drawing\n",
    "            bbox_img = img.copy()\n",
    "            \n",
    "            # Mark the most relevant elements\n",
    "            for i, (x, y, w, h) in enumerate(ui_elements[:3]):\n",
    "                cv2.rectangle(bbox_img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                cv2.putText(bbox_img, \n",
    "                            f\"Step {step_num} Element\", \n",
    "                            (x, y-10), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "    \n",
    "            # Add step text overlay\n",
    "            add_step_instruction(bbox_img, step_num, actions, bbox_img.shape[0])\n",
    "    \n",
    "            # Save the bounding box image\n",
    "            bbox_path = os.path.join(bbox_folder, f\"step{step_num}_bbox.jpg\")\n",
    "            cv2.imwrite(bbox_path, bbox_img)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing step {step_num}: {e}\")\n",
    "    \n",
    "    # Print all actions and thoughts pairs at the end\n",
    "    print(\"\\nAction and Thoughts History:\")\n",
    "    for i in range(0, len(action_history), 2):\n",
    "        print(f\"{action_history[i]}\")\n",
    "        print(f\"{action_history[i + 1]}\")\n",
    "    \n",
    "    print(f\"Enhanced bounding box images saved to {bbox_folder}\")\n",
    "    print(\"\\nStep-by-step instructions:\")\n",
    "    print(cleaned_instructions)\n",
    "    \n",
    "    # Return instructions and action history\n",
    "    return cleaned_instructions, frames, frame_paths, action_history\n",
    "\n",
    "def detect_enhanced_ui_elements(image, step_text, keywords):\n",
    "    \"\"\"\n",
    "    Detect UI elements based on step text and keywords.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image\n",
    "        step_text (str): Step description\n",
    "        keywords (list): Interaction keywords\n",
    "    \n",
    "    Returns:\n",
    "        list: List of detected UI element bounding boxes\n",
    "    \"\"\"\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    # Detect relevant keywords in the step\n",
    "    relevant_keywords = [kw for kw in keywords if kw.lower() in step_text.lower()]\n",
    "    \n",
    "    # If no relevant keywords, return empty list\n",
    "    if not relevant_keywords:\n",
    "        return []\n",
    "    \n",
    "    # Adaptive thresholding\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        gray, 255, \n",
    "        cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "        cv2.THRESH_BINARY_INV, 11, 2\n",
    "    )\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(\n",
    "        thresh, \n",
    "        cv2.RETR_EXTERNAL, \n",
    "        cv2.CHAIN_APPROX_SIMPLE\n",
    "    )\n",
    "    \n",
    "    # Filter contours based on size and aspect ratio\n",
    "    elements = []\n",
    "    min_area = (width * height) * 0.001  # Minimum area\n",
    "    max_area = (width * height) * 0.2    # Maximum area\n",
    "    \n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if min_area < area < max_area:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            \n",
    "            # Filter by aspect ratio\n",
    "            aspect_ratio = float(w) / h if h > 0 else 0\n",
    "            if 0.2 < aspect_ratio < 5:\n",
    "                # Additional checks based on keywords\n",
    "                if any(keyword in ['button', 'icon', 'menu', 'option'] for keyword in relevant_keywords):\n",
    "                    # Check for rectangular shapes more typical of UI elements\n",
    "                    if 0.5 < aspect_ratio < 3:\n",
    "                        elements.append((x, y, w, h))\n",
    "                else:\n",
    "                    elements.append((x, y, w, h))\n",
    "    \n",
    "    return elements\n",
    "\n",
    "def detect_ui_elements(image):\n",
    "    \"\"\"\n",
    "    Detect UI elements in the image.\n",
    "    (Kept the same as in the previous implementation)\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    elements = []\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply adaptive thresholding\n",
    "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Filter contours by size to find potential UI elements\n",
    "    min_area = (width * height) * 0.001  # Minimum area threshold\n",
    "    max_area = (width * height) * 0.1    # Maximum area threshold\n",
    "    \n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if min_area < area < max_area:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            \n",
    "            # Check if it's likely a UI element based on aspect ratio\n",
    "            aspect_ratio = float(w) / h if h > 0 else 0\n",
    "            if 0.2 < aspect_ratio < 5:  # Common UI element aspect ratios\n",
    "                elements.append((x, y, w, h))\n",
    "    \n",
    "    return elements\n",
    "\n",
    "def add_step_instruction(image, step_num, step_text, height):\n",
    "    \"\"\"Add the step instruction as an overlay at the bottom of the image.\"\"\"\n",
    "    # Create semi-transparent overlay for text background\n",
    "    overlay = image.copy()\n",
    "    cv2.rectangle(overlay, (0, height-80), (image.shape[1], height), (0, 0, 0), -1)\n",
    "    cv2.addWeighted(overlay, 0.7, image, 0.3, 0, image)\n",
    "    \n",
    "    # Clean and truncate the step text\n",
    "    clean_text = re.sub(r'\\([^)]*\\)', '', step_text).strip()  # Remove coordinate info\n",
    "    truncated_text = clean_text[:80] + \"...\" if len(clean_text) > 80 else clean_text\n",
    "    \n",
    "    # Add text\n",
    "    cv2.putText(image, \n",
    "               f\"Step {step_num}: {truncated_text}\", \n",
    "               (10, height-40),\n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    os_atlas_enhanced_bounding_boxes(query_folder, query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
