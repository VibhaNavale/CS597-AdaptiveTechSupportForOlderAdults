{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d0613eb-dae6-4a59-a28e-373e42c8d674",
   "metadata": {},
   "source": [
    "## Adaptive Tech Support for Older Adults\n",
    "\n",
    "This is an automated system that generates step-by-step tech support guides with screenshots for older adults based on user queries viaprompt chaining. By retrieving relevant YouTube tutorials via the YouTube API, extracting key UI elements using OpenCV, and creating visual guides with the OS-ATLAS action model, the system aims to enhance the accessibility and effectiveness of tech support for older adults by leveraging video tutorial content in a more accessible, image-based format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21e24df-48b0-4321-8f97-f9f3c8f59ead",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1. Installation of Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934f63cf-1850-4276-ba7c-f8c39ddf0245",
   "metadata": {},
   "source": [
    "This cell installs the necessary Python libraries for the project, including packages for Google API access, video processing, image manipulation, machine learning, and deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77269496-4d4e-41f5-bd1c-8e31165b2db0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client\n",
    "!pip install opencv-python-headless yt-dlp\n",
    "!pip install scikit-image\n",
    "!pip install qwen_vl_utils\n",
    "!pip install transformers\n",
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401fc256-5522-4569-ac40-f9ad7b422710",
   "metadata": {},
   "source": [
    "### 2. YouTube Video Search and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae02153-5b34-4fe4-89ef-41197540faf4",
   "metadata": {},
   "source": [
    "This cell performs the following tasks:\n",
    "- **Searches for YouTube videos** based on a user-provided query using the YouTube Data API.\n",
    "- **Cleans the query** to make it folder-friendly (removes spaces and special characters).\n",
    "- **Fetches video details** including title, URL, and view count.\n",
    "- **Selects the video with the highest number of views** from the search results.\n",
    "- The user is prompted to enter a search query, and the most popular video is displayed with its title, URL, and view count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f28b3e-9062-4110-842a-80687c621c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "# YouTube API key\n",
    "YOUTUBE_API_KEY = \"\" # Add your API Key here\n",
    "\n",
    "def clean_query(query):\n",
    "    \"\"\"Convert query to a folder-friendly format (remove spaces & special characters).\"\"\"\n",
    "    return re.sub(r'\\W+', '_', query.strip())\n",
    "\n",
    "def search_youtube_video(query, max_results=5, max_duration_seconds=120):\n",
    "    search_url = \"https://www.googleapis.com/youtube/v3/search\"\n",
    "    search_params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"q\": query,\n",
    "        \"type\": \"video\",\n",
    "        \"maxResults\": max_results,\n",
    "        \"key\": YOUTUBE_API_KEY,\n",
    "        \"relevanceLanguage\": \"en\",  # Prioritize relevance in search results\n",
    "        \"order\": \"relevance\"  # Sort by relevance\n",
    "    }\n",
    "    response = requests.get(search_url, params=search_params).json()\n",
    "    \n",
    "    # Extract video IDs\n",
    "    video_results = []\n",
    "    video_ids = []\n",
    "    for item in response.get(\"items\", []):\n",
    "        video_id = item[\"id\"][\"videoId\"]\n",
    "        title = item[\"snippet\"][\"title\"]\n",
    "        video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "        video_ids.append(video_id)\n",
    "        video_results.append({\"id\": video_id, \"title\": title, \"url\": video_url})\n",
    "    \n",
    "    if not video_results:\n",
    "        return None\n",
    "    \n",
    "    # Fetch video content details (for duration) and statistics (for views)\n",
    "    details_url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "    details_params = {\n",
    "        \"part\": \"contentDetails,statistics\",\n",
    "        \"id\": \",\".join(video_ids),\n",
    "        \"key\": YOUTUBE_API_KEY\n",
    "    }\n",
    "    details_response = requests.get(details_url, params=details_params).json()\n",
    "    \n",
    "    # Process videos with duration and views information\n",
    "    valid_videos = []\n",
    "    for i, item in enumerate(details_response.get(\"items\", [])):\n",
    "        # Parse duration (in ISO 8601 format, e.g., PT1M30S for 1 min 30 sec)\n",
    "        duration_str = item[\"contentDetails\"][\"duration\"]\n",
    "        # Extract minutes and seconds\n",
    "        minutes_match = re.search(r'(\\d+)M', duration_str)\n",
    "        seconds_match = re.search(r'(\\d+)S', duration_str)\n",
    "        \n",
    "        minutes = int(minutes_match.group(1)) if minutes_match else 0\n",
    "        seconds = int(seconds_match.group(1)) if seconds_match else 0\n",
    "        \n",
    "        # Calculate total duration in seconds\n",
    "        total_seconds = minutes * 60 + seconds\n",
    "        \n",
    "        # Add to valid videos if under max_duration_seconds\n",
    "        if total_seconds <= max_duration_seconds:\n",
    "            video_results[i][\"duration_seconds\"] = total_seconds\n",
    "            video_results[i][\"views\"] = int(item[\"statistics\"].get(\"viewCount\", 0))\n",
    "            valid_videos.append(video_results[i])\n",
    "    \n",
    "    if not valid_videos:\n",
    "        return None\n",
    "    \n",
    "    # Sort by relevance (using original order) then by views as a tiebreaker\n",
    "    # Since the API already returned videos sorted by relevance, we maintain that order\n",
    "    # but use view count as a secondary sort criterion\n",
    "    best_video = max(valid_videos, key=lambda x: x[\"views\"])\n",
    "    \n",
    "    return best_video\n",
    "\n",
    "# Dynamic user input\n",
    "query = input(\"Enter your search query: \")\n",
    "query_folder = clean_query(query)  # Ensure query is folder-safe\n",
    "best_video = search_youtube_video(query)\n",
    "\n",
    "if best_video:\n",
    "    print(f\"Selected Video: {best_video['title']} - {best_video['url']}\")\n",
    "    print(f\"Duration: {best_video['duration_seconds']} seconds | Views: {best_video['views']}\")\n",
    "else:\n",
    "    print(\"No videos under 2 minutes found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e21f38f-bf39-4402-86b0-cd236d292595",
   "metadata": {},
   "source": [
    "### 3. Video Download and Folder Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79184554-0357-4b95-bef3-ca557a4294a6",
   "metadata": {},
   "source": [
    "This cell performs the following tasks:\n",
    "- **Clears cached data** from yt-dlp to ensure fresh downloads.\n",
    "- **Sets up folders** for storing the video and screenshots related to the search query:\n",
    "  - Deletes any existing folders and creates new ones.\n",
    "- **Downloads the selected YouTube video**:\n",
    "  - The video is saved in the newly created folder for the search query.\n",
    "  - The `yt-dlp` tool is configured to download the best available quality.\n",
    "  - A cookie file (if available) is used for the download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca0d113-33fa-4794-ae10-8118baed4e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "yt_dlp.YoutubeDL().cache.remove()\n",
    "# if os.path.exists('cookies.txt'):\n",
    "#     os.remove('cookies.txt')\n",
    "\n",
    "def setup_folders(query_folder):\n",
    "    \"\"\"Delete old data & create new folders for the query.\"\"\"\n",
    "    base_dirs = [\"videos\", \"screenshots\"]\n",
    "    for base in base_dirs:\n",
    "        folder_path = os.path.join(base, query_folder)\n",
    "        if os.path.exists(folder_path):\n",
    "            shutil.rmtree(folder_path)  # Delete old folder\n",
    "        os.makedirs(folder_path)  # Create fresh folder\n",
    "\n",
    "def download_video(video_url, output_folder=\"videos\"):\n",
    "    query_video_folder = os.path.join(output_folder, query_folder)\n",
    "    os.makedirs(query_video_folder, exist_ok=True)  # Ensure folder exists\n",
    "    ydl_opts = {\n",
    "        \"format\": \"best\",\n",
    "        \"outtmpl\": f\"{query_video_folder}/%(id)s.%(ext)s\",\n",
    "        \"cookiefile\": \"cookies.txt\",\n",
    "        \"force-ipv4\": True,\n",
    "        # \"compat_opts\": [\"legacy-server\"]\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.cache.remove()\n",
    "        ydl.download([video_url])\n",
    "\n",
    "# Setup folders for new query\n",
    "setup_folders(query_folder)\n",
    "# Use the video selected previously\n",
    "download_video(best_video[\"url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a554ad21-5cb3-42ec-a581-eca9e9321f4d",
   "metadata": {},
   "source": [
    "### 4. Phone Screen Detection and Frame Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c738cf46-9e57-43f1-a1f7-deac7a324d4e",
   "metadata": {},
   "source": [
    "This cell performs the following tasks:\n",
    "- **Detects the phone screen** in video frames using edge detection and contour approximation:\n",
    "  - Converts frames to grayscale and uses Canny edge detection.\n",
    "  - Finds the largest contour, approximates it to a polygon, and identifies it as the phone screen if it is a quadrilateral.\n",
    "- **Extracts relevant frames** from the video:\n",
    "  - Frames are taken at intervals, and only the frames where the phone screen is detected are processed.\n",
    "  - Uses Structural Similarity Index (SSIM) to filter out similar frames, saving only frames with noticeable UI changes.\n",
    "  - Screenshots are saved in the specified folder for the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b282303-addd-4c14-a2d6-82b9357a398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def detect_phone_screen(frame):\n",
    "    \"\"\"Detects the largest rectangle (phone screen) in the frame using edge detection and contour approximation.\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if not contours:\n",
    "        return None  # No contours found\n",
    "    \n",
    "    # Find the largest rectangle\n",
    "    max_contour = max(contours, key=cv2.contourArea)\n",
    "    epsilon = 0.02 * cv2.arcLength(max_contour, True)\n",
    "    approx = cv2.approxPolyDP(max_contour, epsilon, True)\n",
    "    \n",
    "    if len(approx) == 4:  # If it's a quadrilateral, assume it's the phone screen\n",
    "        return approx.reshape(4, 2)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_relevant_frames(video_path, output_folder=\"screenshots\", frame_interval=1, similarity_threshold=0.85, min_pixel_change_threshold=0.02):\n",
    "    \"\"\"\n",
    "    Extract frames with significant UI changes from a video.\n",
    "    \n",
    "    :param video_path: Path to the input video file\n",
    "    :param output_folder: Base output folder for screenshots\n",
    "    :param frame_interval: Process every nth frame\n",
    "    :param similarity_threshold: SSIM threshold for UI changes (lower means more changes detected)\n",
    "    :param min_pixel_change_threshold: Minimum percentage of pixels that must change\n",
    "    \"\"\"\n",
    "\n",
    "    # Create query-specific screenshot folder\n",
    "    query_screenshot_folder = os.path.join(output_folder, query_folder)\n",
    "    os.makedirs(query_screenshot_folder, exist_ok=True)  # Ensure folder exists\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    screenshot_count = 0\n",
    "    last_saved_frame = None\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Skip frames to reduce processing time\n",
    "        if frame_count % frame_interval == 0:\n",
    "            phone_screen = detect_phone_screen(frame)\n",
    "            \n",
    "            if phone_screen is not None:  # Only process frames where the phone screen is detected\n",
    "                gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                if last_saved_frame is None:\n",
    "                    # Save first frame\n",
    "                    save_path = os.path.join(query_screenshot_folder, f\"frame_{screenshot_count}.jpg\")\n",
    "                    cv2.imwrite(save_path, frame)\n",
    "                    last_saved_frame = gray_frame\n",
    "                    screenshot_count += 1\n",
    "                else:\n",
    "                    # Compare with last saved frame\n",
    "                    similarity = ssim(last_saved_frame, gray_frame)\n",
    "                    \n",
    "                    # Calculate pixel change percentage\n",
    "                    pixel_diff = np.sum(np.abs(last_saved_frame.astype(int) - gray_frame.astype(int))) / (gray_frame.shape[0] * gray_frame.shape[1] * 255)\n",
    "                    \n",
    "                    # Save frame if significant UI change detected\n",
    "                    if (similarity < similarity_threshold) or (pixel_diff > min_pixel_change_threshold):\n",
    "                        save_path = os.path.join(query_screenshot_folder, f\"frame_{screenshot_count}.jpg\")\n",
    "                        cv2.imwrite(save_path, frame)\n",
    "                        last_saved_frame = gray_frame\n",
    "                        screenshot_count += 1\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    print(f\"Extracted {screenshot_count} relevant UI screenshots in '{query_screenshot_folder}'.\")\n",
    "    return screenshot_count\n",
    "\n",
    "# Example usage\n",
    "video_file = f\"videos/{query_folder}/{best_video['id']}.mp4\"\n",
    "extract_relevant_frames(video_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46b4bf7-b312-4eda-b62e-32ce7cf420e8",
   "metadata": {},
   "source": [
    "### 5. UI Screen Detection and Cropping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054fa68d-65fd-4c4a-92a8-eaaa38e6a525",
   "metadata": {},
   "source": [
    "This cell contains functions for detecting and extracting the screen region from images, using methods like adaptive thresholding, Canny edge detection, and contour analysis. It includes:\n",
    "- `detect_phone_screen(image)`: Detects the phone's screen area by analyzing edges, contours, and aspect ratios.\n",
    "- `crop_screen_with_perspective(image, screen_contour)`: Crops the detected screen area using a perspective transformation.\n",
    "- `order_points(pts)`: Orders the contour points to ensure correct perspective transformation.\n",
    "- `visualize_detection(image, screen_contour, filename=None, output_dir=None)`: Visualizes the detected screen contour for debugging.\n",
    "- `extract_ui_screenshots(input_folder=\"screenshots\", output_folder=\"ui-screens\")`: Extracts and saves only the phone screen regions from screenshots.\n",
    "\n",
    "These functions help isolate the phone screen from images and ensure correct transformations for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88b613e-e25e-4f59-8a91-2c5a2ce6b590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def detect_phone_screen(image):\n",
    "    \"\"\"Detects the phone in the image, focusing on the likely phone region.\"\"\"\n",
    "    # Create a copy of the image\n",
    "    original = image.copy()\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # 1. Try adaptive thresholding to find the screen\n",
    "    thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "    \n",
    "    # 2. Also try Canny edge detection\n",
    "    canny = cv2.Canny(blurred, 50, 150)\n",
    "    \n",
    "    # Combine both methods\n",
    "    edges = cv2.bitwise_or(thresh, canny)\n",
    "    \n",
    "    # Dilate to connect edges\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    dilated = cv2.dilate(edges, kernel, iterations=1)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(dilated.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Sort contours by area in descending order\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "    \n",
    "    # Additional filter for phone-like aspect ratio\n",
    "    phone_contours = []\n",
    "    \n",
    "    for contour in contours[:10]:  # Only check the 10 largest contours\n",
    "        # Approximate the contour\n",
    "        peri = cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, 0.02 * peri, True)\n",
    "        \n",
    "        # Get bounding rectangle\n",
    "        x, y, w, h = cv2.boundingRect(approx)\n",
    "        \n",
    "        # Skip if the contour is too small\n",
    "        if w * h < (width * height) / 20:\n",
    "            continue\n",
    "            \n",
    "        # Check if it's at least somewhat rectangular (4-ish points)\n",
    "        if len(approx) >= 4 and len(approx) <= 8:\n",
    "            # Check if it has phone-like aspect ratio (height > width)\n",
    "            aspect_ratio = h / w if w > 0 else 0\n",
    "            \n",
    "            # Phones are typically taller than they are wide (aspect ratio > 1.5)\n",
    "            if aspect_ratio > 1.5 and aspect_ratio < 2.5:\n",
    "                phone_contours.append(approx)\n",
    "    \n",
    "    # If we found potential phone contours, use the largest one\n",
    "    if phone_contours:\n",
    "        largest_phone_contour = max(phone_contours, key=cv2.contourArea)\n",
    "        # Get the bounding rectangle\n",
    "        x, y, w, h = cv2.boundingRect(largest_phone_contour)\n",
    "        \n",
    "        # Create a 4-point contour from the rectangle for perspective transform\n",
    "        screen_contour = np.array([\n",
    "            [x, y],           # Top-left\n",
    "            [x + w, y],       # Top-right\n",
    "            [x + w, y + h],   # Bottom-right\n",
    "            [x, y + h]        # Bottom-left\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        return screen_contour\n",
    "    \n",
    "    # If no phone contour found, look for a rectangle with phone-like proportions\n",
    "    # in the center half of the image (where phones usually are in tutorial videos)\n",
    "    \n",
    "    # Define the center region (middle 50% of the image)\n",
    "    center_x = width // 4\n",
    "    center_y = height // 4\n",
    "    center_w = width // 2\n",
    "    center_h = height // 2\n",
    "    \n",
    "    # Create a mask for the center region\n",
    "    mask = np.zeros_like(gray)\n",
    "    mask[center_y:center_y+center_h, center_x:center_x+center_w] = 255\n",
    "    \n",
    "    # Apply the mask to the edge image\n",
    "    masked_edges = cv2.bitwise_and(dilated, mask)\n",
    "    \n",
    "    # Find contours in the center region\n",
    "    center_contours, _ = cv2.findContours(masked_edges, cv2.RETR_EXTERNAL, \n",
    "                                         cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Sort by area\n",
    "    center_contours = sorted(center_contours, key=cv2.contourArea, reverse=True)\n",
    "    \n",
    "    for contour in center_contours[:5]:\n",
    "        # Approximate the contour\n",
    "        peri = cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, 0.02 * peri, True)\n",
    "        \n",
    "        # Get bounding rectangle\n",
    "        x, y, w, h = cv2.boundingRect(approx)\n",
    "        \n",
    "        # Skip small contours\n",
    "        if w * h < (width * height) / 30:\n",
    "            continue\n",
    "            \n",
    "        # Check aspect ratio (phone-like)\n",
    "        aspect_ratio = h / w if w > 0 else 0\n",
    "        if aspect_ratio > 1.5 and aspect_ratio < 2.5:\n",
    "            # Create a 4-point contour\n",
    "            screen_contour = np.array([\n",
    "                [x, y],           # Top-left\n",
    "                [x + w, y],       # Top-right\n",
    "                [x + w, y + h],   # Bottom-right\n",
    "                [x, y + h]        # Bottom-left\n",
    "            ], dtype=np.float32)\n",
    "            \n",
    "            return screen_contour\n",
    "    \n",
    "    # If we still haven't found anything, try one more approach:\n",
    "    # Find the most central large rectangle in the image\n",
    "    \n",
    "    # Use Hough Line transform to find straight lines\n",
    "    lines = cv2.HoughLinesP(edges, 1, np.pi/180, 100, minLineLength=100, maxLineGap=10)\n",
    "    \n",
    "    if lines is not None:\n",
    "        # Draw the lines on a blank image\n",
    "        line_image = np.zeros_like(gray)\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            cv2.line(line_image, (x1, y1), (x2, y2), 255, 2)\n",
    "        \n",
    "        # Find contours in the line image\n",
    "        line_contours, _ = cv2.findContours(line_image, cv2.RETR_EXTERNAL, \n",
    "                                           cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # Find the contour closest to the center of the image\n",
    "        image_center = np.array([width/2, height/2])\n",
    "        best_distance = float('inf')\n",
    "        best_contour = None\n",
    "        \n",
    "        for contour in line_contours:\n",
    "            if cv2.contourArea(contour) < (width * height) / 20:\n",
    "                continue\n",
    "                \n",
    "            # Find center of contour\n",
    "            M = cv2.moments(contour)\n",
    "            if M[\"m00\"] != 0:\n",
    "                cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "                contour_center = np.array([cx, cy])\n",
    "                \n",
    "                # Calculate distance to image center\n",
    "                distance = np.linalg.norm(contour_center - image_center)\n",
    "                \n",
    "                # Get bounding rect to check aspect ratio\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                aspect_ratio = h / w if w > 0 else 0\n",
    "                \n",
    "                # If it's phone-like and closer to center than previous best\n",
    "                if aspect_ratio > 1.5 and aspect_ratio < 2.5 and distance < best_distance:\n",
    "                    best_distance = distance\n",
    "                    best_contour = contour\n",
    "        \n",
    "        if best_contour is not None:\n",
    "            # Get the bounding rectangle\n",
    "            x, y, w, h = cv2.boundingRect(best_contour)\n",
    "            \n",
    "            # Create a 4-point contour\n",
    "            screen_contour = np.array([\n",
    "                [x, y],           # Top-left\n",
    "                [x + w, y],       # Top-right\n",
    "                [x + w, y + h],   # Bottom-right\n",
    "                [x, y + h]        # Bottom-left\n",
    "            ], dtype=np.float32)\n",
    "            \n",
    "            return screen_contour\n",
    "    \n",
    "    # If all else fails, just take the center portion of the image \n",
    "    # with a typical phone aspect ratio (this is a fallback)\n",
    "    center_x = width // 3\n",
    "    center_width = width // 3\n",
    "    \n",
    "    # Calculate height based on typical phone aspect ratio (1:2)\n",
    "    center_height = center_width * 2\n",
    "    center_y = (height - center_height) // 2\n",
    "    \n",
    "    # Create a rectangle\n",
    "    screen_contour = np.array([\n",
    "        [center_x, center_y],                       # Top-left\n",
    "        [center_x + center_width, center_y],        # Top-right\n",
    "        [center_x + center_width, center_y + center_height],  # Bottom-right\n",
    "        [center_x, center_y + center_height]        # Bottom-left\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    return screen_contour\n",
    "\n",
    "def crop_screen_with_perspective(image, screen_contour):\n",
    "    \"\"\"Crops the region inside the screen contour using a perspective transform without resizing.\"\"\"\n",
    "    if screen_contour is None or len(screen_contour) != 4:\n",
    "        return None\n",
    "\n",
    "    # Order points in a consistent order (top-left, top-right, bottom-right, bottom-left)\n",
    "    ordered_points = order_points(screen_contour)\n",
    "\n",
    "    # Compute the width and height dynamically\n",
    "    width = int(np.linalg.norm(ordered_points[1] - ordered_points[0]))  # Distance between top-left & top-right\n",
    "    height = int(np.linalg.norm(ordered_points[2] - ordered_points[1]))  # Distance between top-right & bottom-right\n",
    "\n",
    "    # Define the destination points dynamically based on the detected screen dimensions\n",
    "    dst_points = np.array([\n",
    "        [0, 0],\n",
    "        [width - 1, 0],\n",
    "        [width - 1, height - 1],\n",
    "        [0, height - 1]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # Compute the perspective transform matrix\n",
    "    matrix = cv2.getPerspectiveTransform(ordered_points, dst_points)\n",
    "\n",
    "    # Warp the perspective to get the cropped screen\n",
    "    cropped = cv2.warpPerspective(image, matrix, (width, height))\n",
    "\n",
    "    return cropped\n",
    "\n",
    "    # Resize the cropped image to the desired size (e.g., 1024x1024)\n",
    "    # resized_cropped = cv2.resize(cropped, (768, 2000), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # return resized_cropped\n",
    "\n",
    "def order_points(pts):\n",
    "    \"\"\"Orders points in top-left, top-right, bottom-right, bottom-left order.\"\"\"\n",
    "    # Sort by y-coordinate (top to bottom)\n",
    "    sorted_by_y = pts[np.argsort(pts[:, 1])]\n",
    "    \n",
    "    # Get top and bottom points\n",
    "    top_points = sorted_by_y[:2]\n",
    "    bottom_points = sorted_by_y[2:]\n",
    "    \n",
    "    # Sort top points by x-coordinate (left to right)\n",
    "    top_left, top_right = top_points[np.argsort(top_points[:, 0])]\n",
    "    \n",
    "    # Sort bottom points by x-coordinate (left to right)\n",
    "    bottom_left, bottom_right = bottom_points[np.argsort(bottom_points[:, 0])]\n",
    "    \n",
    "    # Return points in order: top-left, top-right, bottom-right, bottom-left\n",
    "    return np.array([top_left, top_right, bottom_right, bottom_left], dtype=np.float32)\n",
    "\n",
    "def visualize_detection(image, screen_contour, filename=None, output_dir=None):\n",
    "    \"\"\"Visualizes the detected screen contour for debugging.\"\"\"\n",
    "    vis = image.copy()\n",
    "    \n",
    "    if screen_contour is not None:\n",
    "        # Draw the contour\n",
    "        cv2.drawContours(vis, [screen_contour.astype(int)], -1, (0, 255, 0), 2)\n",
    "        \n",
    "        # Draw the ordered corners\n",
    "        rect = order_points(screen_contour)\n",
    "        colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0)]  # BGR for corners\n",
    "        labels = [\"TL\", \"TR\", \"BR\", \"BL\"]  # Corner labels\n",
    "        \n",
    "        for i, (x, y) in enumerate(rect.astype(int)):\n",
    "            cv2.circle(vis, (x, y), 8, colors[i], -1)\n",
    "            cv2.putText(vis, labels[i], (x - 10, y - 10), \n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.8, colors[i], 2)\n",
    "    \n",
    "    if filename and output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        save_path = os.path.join(output_dir, f\"debug_{filename}\")\n",
    "        cv2.imwrite(save_path, vis)\n",
    "    \n",
    "    return vis\n",
    "\n",
    "def extract_ui_screenshots(input_folder=\"screenshots\", output_folder=\"ui-screens\"):\n",
    "    \"\"\"Extracts only the screen region from screenshots and saves the raw cropped area.\"\"\"\n",
    "    query_ui_folder = os.path.join(output_folder, query_folder)\n",
    "    debug_folder = os.path.join(output_folder, \"debug\")\n",
    "    \n",
    "    os.makedirs(query_ui_folder, exist_ok=True)\n",
    "    os.makedirs(debug_folder, exist_ok=True)\n",
    "    \n",
    "    screenshot_files = sorted(os.listdir(os.path.join(input_folder, query_folder)))\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for screenshot in screenshot_files:\n",
    "        img_path = os.path.join(input_folder, query_folder, screenshot)\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        if img is None:\n",
    "            # print(f\"Could not read image: {img_path}\")\n",
    "            failed += 1\n",
    "            continue\n",
    "        \n",
    "        # Detect phone screen\n",
    "        screen_contour = detect_phone_screen(img)\n",
    "        \n",
    "        # Create visualization for debugging\n",
    "        visualize_detection(img, screen_contour, screenshot, debug_folder)\n",
    "        \n",
    "        if screen_contour is not None:\n",
    "            # Simply crop the region inside the green box without perspective correction\n",
    "            cropped_screen = crop_screen_with_perspective(img, screen_contour)\n",
    "            \n",
    "            if cropped_screen is not None and cropped_screen.size > 0:\n",
    "                save_path = os.path.join(query_ui_folder, screenshot)\n",
    "                cv2.imwrite(save_path, cropped_screen)\n",
    "                # print(f\"Successfully processed: {screenshot}\")\n",
    "                successful += 1\n",
    "            else:\n",
    "                # print(f\"Failed to process: {screenshot} - Invalid crop\")\n",
    "                failed += 1\n",
    "        else:\n",
    "            print(f\"✗ Failed to detect screen in: {screenshot}\")\n",
    "            failed += 1\n",
    "    \n",
    "    print(f\"Extraction complete!\")\n",
    "    print(f\"Successfully processed: {successful} images\")\n",
    "    print(f\"Failed to process: {failed} images\")\n",
    "    print(f\"Extracted UI screens saved in '{query_ui_folder}'\")\n",
    "    print(f\"Debug visualizations saved in '{debug_folder}'\")\n",
    "\n",
    "# Run UI screen extraction\n",
    "extract_ui_screenshots()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9528398a-30d6-48de-946f-ce380081cd6f",
   "metadata": {},
   "source": [
    "### 6. OS-Atlas Action Model for UI Bounding Box Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3ca2a2-9dda-49ed-b540-e34f1870fbf6",
   "metadata": {},
   "source": [
    "This cell processes UI screenshots to generate step-by-step instructions for a user query, such as a technical support task. It processes a series of UI screenshots, performs action history tracking, and generates step-by-step instructions with bounding boxes for UI elements.\n",
    "\n",
    "- Bounding box generation\n",
    "- Interaction keyword detection (e.g., button, text, icon, etc.)\n",
    "- Step-by-step instructions generation for UI actions\n",
    "- Action history tracking\n",
    "- GPU memory management\n",
    "- Uses the `Qwen2VLForConditionalGeneration` model from Hugging Face\n",
    "\n",
    "This script is used for extracting detailed UI instructions, enhancing bounding boxes, and providing a more refined view of the UI interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f755ffce-0ef0-4fe1-9736-37762ec5b74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS-ATLAS - ACTION MODEL\n",
    "\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import torch\n",
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# Clean up GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "# Load the model and processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"OS-Copilot/OS-Atlas-Pro-7B\", torch_dtype=\"auto\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"OS-Copilot/OS-Atlas-Pro-7B\",\n",
    "    size=None\n",
    ")\n",
    "\n",
    "# Define the system prompt for Technology Support mode for older adults\n",
    "sys_prompt = \"\"\"  \n",
    "You are in Technology Support mode for older adults. Your role is to assist users over 60 with technology issues by providing step-by-step executable actions.  \n",
    "\n",
    "Guidelines:  \n",
    "1. Step-by-Step Instructions:  \n",
    "    - Provide clear, short steps that are easy to follow.  \n",
    "    - Every action must be directly executable without assumptions.  \n",
    "    - Example: Instead of \"Go to settings,\" specify each step to navigate there.\n",
    "    - Use the images for the step generation:\n",
    "        - Exclude or skip over intro, outro and unclear images.\n",
    "        - Do not use images that do not have interactive UI elements.\n",
    "        - Strict guidelines: ALWAYS generate coordinates relative to the image's size and resolution:\n",
    "            - The coordinates need to be within the image bounds.\n",
    "            - The coordinates need to be correct and accurate location for the UI element mentioned in each step (in 'thought').\n",
    "            - The coordinates should be in abosolute pixel values instead of normalized values. Image resolution is: heightxwidth taken from the `image.shape`.\n",
    "\n",
    "2. Strict Action Format:  \n",
    "    - Each step must have:  \n",
    "        - Thought: Explains the reason for the next action.  \n",
    "        - Action: Specifies what to do in a predefined format.\n",
    "\n",
    "4. No Follow-Up Questions:  \n",
    "    - Do not ask for clarification.  \n",
    "    - Use only given screenshots and action history.\n",
    "\n",
    "Action Formats:  \n",
    "1. CLICK: Click on a position. Format: CLICK <point>[x, y]</point>  \n",
    "2. TYPE: Enter text. Format: TYPE [input text]  \n",
    "3. SCROLL: Scroll in a direction. Format: SCROLL [UP/DOWN/LEFT/RIGHT]  \n",
    "4. OPEN_APP: Open an app. Format: OPEN_APP [app_name]  \n",
    "5. PRESS_BACK: Go to the previous screen. Format: PRESS_BACK  \n",
    "6. PRESS_HOME: Return to the home screen. Format: PRESS_HOME  \n",
    "7. ENTER: Press enter. Format: ENTER  \n",
    "8. WAIT: Pause for loading. Format: WAIT  \n",
    "9. COMPLETE: Task finished. Format: COMPLETE  \n",
    "\n",
    "Example Response:  \n",
    "Query: \"How do I take a screenshot on my iPhone?\"\n",
    "\n",
    "Thought: Open the screen you want to capture.\n",
    "Action: OPEN_APP [Gallery]\n",
    "\n",
    "Thought: Press the correct button combination to take a screenshot.\n",
    "Action: PRESS [Power + Volume Up]\n",
    "\n",
    "Thought: The screenshot is captured and saved.\n",
    "Action: COMPLETE  \n",
    "\"\"\"\n",
    "\n",
    "def extract_coordinates(action, image_width, image_height):\n",
    "    \"\"\"\n",
    "    Extracts normalized coordinates from the action and converts them\n",
    "    to absolute pixel values based on the image dimensions.\n",
    "    \"\"\"\n",
    "    # Patterns to try for coordinate extraction\n",
    "    patterns = [\n",
    "        r'<point>\\[\\[(\\d+),(\\d+)\\]\\]</point>',  # Original pattern with XML-like tags\n",
    "        r'\\[\\[(\\d+),(\\d+)\\]\\]',                 # Nested list format\n",
    "        r'<point>\\[(\\d+),(\\d+)\\]</point>',      # Alternative XML-like tag\n",
    "        r'\\[(\\d+),(\\d+)\\]'                      # Simple list format\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, action)\n",
    "        if match:\n",
    "            x, y = map(int, match.groups())\n",
    "            \n",
    "            # Convert normalized coordinates to absolute pixel values\n",
    "            abs_x = int(x * image_width / 1000)  # Convert normalized x to pixels\n",
    "            abs_y = int(y * image_height / 1000)  # Convert normalized y to pixels\n",
    "\n",
    "            print(f\"Extracted coordinates (normalized): ({x}, {y})\")\n",
    "            print(f\"Converted to absolute pixel values: ({abs_x}, {abs_y})\")\n",
    "            return abs_x, abs_y\n",
    "    \n",
    "    # If no coordinates found, print the full action for debugging\n",
    "    print(f\"No coordinates found in action: {action}\")\n",
    "    return None\n",
    "\n",
    "def draw_bounding_box(image, coordinates, step_number, action_type):\n",
    "    \"\"\"\n",
    "    Draws a bounding box dynamically around the UI element based on a calculated region.\n",
    "    \"\"\"\n",
    "    if coordinates is None:\n",
    "        return image\n",
    "    \n",
    "    x, y = coordinates\n",
    "    \n",
    "    # Validate coordinates are within image bounds\n",
    "    height, width = image.shape[:2]\n",
    "    if x < 0 or x >= width or y < 0 or y >= height:\n",
    "        print(f\"Warning: Coordinates ({x}, {y}) are outside image bounds ({width}x{height})\")\n",
    "        return image\n",
    "    \n",
    "    # Create a copy of the image to draw on\n",
    "    img_with_box = image.copy()\n",
    "    \n",
    "    # Define a dynamic bounding box size based on a percentage of image dimensions\n",
    "    box_size_x = int(width * 0.2)  # 30% of image width for bounding box size\n",
    "    box_size_y = int(height * 0.2)  # 30% of image height for bounding box size\n",
    "    \n",
    "    # Apply a margin around the coordinates for better bounding box positioning\n",
    "    margin = 0.05  # 10% margin to expand the bounding box\n",
    "    expanded_box_size_x = int(box_size_x * (1 + margin))\n",
    "    expanded_box_size_y = int(box_size_y * (1 + margin))\n",
    "    \n",
    "    # Calculate box coordinates with expanded size\n",
    "    top_left = (\n",
    "        max(0, x - expanded_box_size_x // 2), \n",
    "        max(0, y - expanded_box_size_y // 2)\n",
    "    )\n",
    "    bottom_right = (\n",
    "        min(width, x + expanded_box_size_x // 2), \n",
    "        min(height, y + expanded_box_size_y // 2)\n",
    "    )\n",
    "    \n",
    "    # Choose color based on action type\n",
    "    if 'CLICK' in action_type.upper():\n",
    "        color = (0, 255, 0)  # Green for CLICK\n",
    "    elif 'TYPE' in action_type.upper():\n",
    "        color = (255, 0, 0)  # Blue for TYPE\n",
    "    elif 'SCROLL' in action_type.upper():\n",
    "        color = (255, 165, 0)  # Orange for SCROLL\n",
    "    else:\n",
    "        color = (0, 0, 255)  # Red for other actions\n",
    "    \n",
    "    # Draw the rectangle (bounding box)\n",
    "    cv2.rectangle(img_with_box, top_left, bottom_right, color, thickness=2)\n",
    "    \n",
    "    # Add step number text with better text placement\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = min(width, height) / 500  # Dynamic font scaling\n",
    "    font_thickness = 1\n",
    "    \n",
    "    text = f\"Step {step_number}\"\n",
    "    text_size = cv2.getTextSize(text, font, font_scale, font_thickness)[0]\n",
    "    text_pos = (\n",
    "        max(0, top_left[0]), \n",
    "        max(text_size[1] + 10, top_left[1] - 10)\n",
    "    )\n",
    "    \n",
    "    # Add text with a semi-transparent background for better readability\n",
    "    overlay = img_with_box.copy()\n",
    "    cv2.rectangle(\n",
    "        overlay, \n",
    "        (text_pos[0], text_pos[1] - text_size[1] - 10),\n",
    "        (text_pos[0] + text_size[0] + 10, text_pos[1] + 10), \n",
    "        (255, 255, 255), \n",
    "        -1\n",
    "    )\n",
    "    cv2.addWeighted(overlay, 0.5, img_with_box, 0.5, 0, img_with_box)\n",
    "    \n",
    "    # Draw text\n",
    "    cv2.putText(\n",
    "        img_with_box,\n",
    "        text,\n",
    "        text_pos,\n",
    "        font,\n",
    "        font_scale,\n",
    "        color,\n",
    "        font_thickness\n",
    "    )\n",
    "    \n",
    "    return img_with_box\n",
    "\n",
    "\n",
    "def log_to_file(message):\n",
    "    # Append the message to instructions.txt\n",
    "    with open('os_atlas_instructions.txt', 'a') as file:\n",
    "        file.write(message + '\\n')\n",
    "\n",
    "\n",
    "# Initialize action history\n",
    "action_history = []\n",
    "\n",
    "# Create output folder for boxed images\n",
    "output_folder = f'./os_atlas_steps/{query_folder}'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get all files starting with 'frame_'\n",
    "frame_files = [f for f in os.listdir(f'./ui-screens/{query_folder}') if f.startswith('frame_')]\n",
    "\n",
    "# Sort the files numerically (by extracting the number from the filename)\n",
    "frame_files.sort(key=lambda f: int(f.split('_')[1].split('.')[0]))  # Split based on 'frame_' and take the number\n",
    "\n",
    "step_number = 1\n",
    "\n",
    "# Loop through the frames and process each one\n",
    "for frame_file in frame_files:\n",
    "    # Print the current frame being processed\n",
    "    print(f\"Processing frame: {frame_file}\")\n",
    "\n",
    "    # Prepare the messages for the current image, using the image path\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": sys_prompt},\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": f'./ui-screens/{query_folder}/{frame_file}',\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": f\"Task instruction: '{query}'\\nHistory: {action_history or 'null'}\"}\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    # Generate output for the current frame\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    # Post-process the output\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "    # Initialize variables\n",
    "    last_step = None\n",
    "    \n",
    "    # Extract thought and action\n",
    "    thought, action = None, None\n",
    "    capturing_thought, capturing_action = False, False  # Flags to track multi-line capturing\n",
    "    temp_thought, temp_action = [], []  # Lists to accumulate multi-line text\n",
    "    \n",
    "    for line in output_text[0].splitlines():\n",
    "        line = line.strip()\n",
    "    \n",
    "        if line.lower().startswith(\"thought:\"):\n",
    "            capturing_thought = True\n",
    "            capturing_action = False  # Stop capturing action if we encounter a new thought\n",
    "            continue  # Move to next line to capture the actual thought\n",
    "    \n",
    "        elif line.lower().startswith(\"actions:\"):\n",
    "            capturing_thought = False\n",
    "            capturing_action = True  # Start capturing action\n",
    "            continue  # Move to next line to capture the actual action\n",
    "    \n",
    "        # Capture multi-line thoughts and actions\n",
    "        if capturing_thought:\n",
    "            temp_thought.append(line)\n",
    "        elif capturing_action:\n",
    "            temp_action.append(line.replace(\"<|im_end|>\", \"\").strip())\n",
    "    \n",
    "    # Join lines to reconstruct the full thought and action\n",
    "    thought = \" \".join(temp_thought).strip() if temp_thought else None\n",
    "    action = \" \".join(temp_action).strip() if temp_action else None\n",
    "    \n",
    "    # Ensure both thought and action exist and check uniqueness\n",
    "    if thought and action:\n",
    "        current_step = (thought, action)\n",
    "    \n",
    "        # Normalize the action for comparison\n",
    "        normalized_action = action.strip().lower()\n",
    "    \n",
    "        # Check if the action has already been performed (in action history)\n",
    "        if normalized_action not in [a.strip().lower() for a in action_history]:\n",
    "            message = f\"Step {step_number}:\"\n",
    "            print(message)\n",
    "            log_to_file(message)\n",
    "            message = f\"Thought: {thought}\"\n",
    "            print(message)\n",
    "            log_to_file(message)\n",
    "            message = f\"Action: {action}\"\n",
    "            print(message)\n",
    "            log_to_file(message)\n",
    "            \n",
    "            # Read the image for bounding box\n",
    "            img_path = f'./ui-screens/{query_folder}/{frame_file}'\n",
    "            img = cv2.imread(img_path)\n",
    "\n",
    "            img_height, img_width, _ = img.shape\n",
    "            \n",
    "            # Check if the action has coordinates\n",
    "            coordinates = extract_coordinates(action, img_width, img_height)\n",
    "\n",
    "            # Create a step-specific folder and copy the frame\n",
    "            step_folder = os.path.join(output_folder, f'step_{step_number}')\n",
    "            os.makedirs(step_folder, exist_ok=True)\n",
    "            \n",
    "            # Draw bounding box for actions with coordinates\n",
    "            if coordinates:\n",
    "                img_with_box = draw_bounding_box(img, coordinates, step_number, action)\n",
    "                # Save the image with bounding box\n",
    "                output_img_path = os.path.join(step_folder, frame_file)\n",
    "                cv2.imwrite(output_img_path, img_with_box)\n",
    "            else:\n",
    "                # If no coordinates, just copy the original image\n",
    "                output_img_path = os.path.join(step_folder, frame_file)\n",
    "                cv2.imwrite(output_img_path, img)\n",
    "\n",
    "            # Save step details as JSON\n",
    "            step_details = {\n",
    "                \"step_number\": step_number,\n",
    "                \"frame\": frame_file,\n",
    "                \"thought\": thought,\n",
    "                \"action\": action,\n",
    "                \"coordinates\": coordinates\n",
    "            }\n",
    "\n",
    "            with open(os.path.join(step_folder, 'step_details.json'), 'w') as f:\n",
    "                json.dump(step_details, f, indent=4)\n",
    "            \n",
    "            action_history.append(action)  # Add only action to the history\n",
    "            last_step = current_step  # Update last step\n",
    "            \n",
    "            print(f\"Action History: {action_history}\\n\")\n",
    "            step_number = step_number + 1\n",
    "    else:\n",
    "        print(\"Thought or action missing for this step.\")\n",
    "        \n",
    "        # Copy the original image to the output folder\n",
    "        img_path = f'./ui-screens/{query_folder}/{frame_file}'\n",
    "        img = cv2.imread(img_path)\n",
    "        output_path = os.path.join(output_folder, frame_file)\n",
    "        cv2.imwrite(output_path, img)\n",
    "\n",
    "        step_number = step_number + 1\n",
    "\n",
    "print(\"–––––– END ––––––\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff37963-bfa0-4cfb-a8c1-a44eb6115a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ae296-d5b1-4aac-b6fc-4b08e8b409ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o model\n",
    "\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import json\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import shutil\n",
    "import difflib\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"\" # Add your API Key here\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Define the system prompt for Technology Support mode for older adults\n",
    "sys_prompt = \"\"\"  \n",
    "You are in Technology Support mode for older adults. Your role is to assist users over 60 with technology issues by providing step-by-step executable actions.  \n",
    "\n",
    "Guidelines:  \n",
    "1. Provide clear, minimal steps to complete the task.\n",
    "2. Eliminate redundant or similar steps.\n",
    "3. Focus on unique, actionable instructions.\n",
    "4. Each step should be distinctly different from previous steps. Only include steps that have important instructions to achieve the goal.\n",
    "5. Use the images for the step generation:\n",
    "    - Exclude or skip over intro, outro and unclear images.\n",
    "    - Do not use images that do not have interactive UI elements.\n",
    "    - Strict guidelines: ALWAYS generate coordinates relative to the image's size and resolution:\n",
    "        - The coordinates need to be within the image bounds.\n",
    "        - The coordinates need to be correct and accurate location for the UI element mentioned in each step (in 'thought').\n",
    "        - The coordinates should be in abosolute pixel values instead of normalized values. Image resolution is: heightxwidth taken from the `image.shape`.\n",
    "\n",
    "6. Include:\n",
    "   - Thought: Direct, concise, crucial reasoning for the action\n",
    "   - Action: Simple, specific, executable instruction\n",
    "   - Coordinates: Precise click, tap or any action location of the UI element in the image (relative to the image size), if applicable\n",
    "7. The response must be a JSON object.\n",
    "8. Once the goal is achieved, i.e., after the action is COMPLETE, end the step generation process.\n",
    "\n",
    "Response format (strict JSON):\n",
    "{\n",
    "    \"thought\": \"Unique reasoning for action given in the image and previous step\",\n",
    "    \"action\": \"Specific executable action associated with the elements in the image\",\n",
    "    \"coordinates\": [[x, y]]\n",
    "}\n",
    "\n",
    "IMPORTANT RESPONSE REQUIREMENTS:\n",
    "- ALWAYS respond in VALID JSON format\n",
    "- NEVER return plain text\n",
    "- If no actionable step is possible, return:\n",
    "{\n",
    "    \"thought\": \"No unique step can be derived from this image\",\n",
    "    \"action\": \"SKIP\",\n",
    "    \"coordinates\": []\n",
    "}\n",
    "\n",
    "For each step, provide:\n",
    "- thought: Concise reasoning for the action\n",
    "- action: Specific UI interaction (e.g., \"CLICK\", \"OPEN_APP\", \"TAP_BUTTON\")\n",
    "- coordinates: Exact click coordinates [x, y] or empty list []\n",
    "\n",
    "Example response:\n",
    "{\n",
    "    \"thought\": \"Open Settings to modify Control Center\",\n",
    "    \"action\": \"OPEN_APP\",\n",
    "    \"coordinates\": [180, 50]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"\n",
    "    Encode an image file to base64 for API transmission\n",
    "    \"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def is_similar_text(text1, text2, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Check if two texts are similar using difflib\n",
    "    \"\"\"\n",
    "    matcher = difflib.SequenceMatcher(None, text1, text2)\n",
    "    return matcher.ratio() > threshold\n",
    "\n",
    "def extract_json(text):\n",
    "    \"\"\"\n",
    "    Extract JSON from text by removing markdown code block markers\n",
    "    \"\"\"\n",
    "    # Remove ```json and ``` markers\n",
    "    text = text.replace('```json', '').replace('```', '').strip()\n",
    "    return text\n",
    "\n",
    "def draw_bounding_boxes(image_path, coordinates):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes around elements in the image based on the given coordinates,\n",
    "    ensuring they remain within image bounds.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to read image {image_path}\")\n",
    "        return None\n",
    "\n",
    "    img_height, img_width, _ = image.shape  # Get image dimensions\n",
    "\n",
    "    # Define bounding box properties dynamically based on image size\n",
    "    box_size = min(img_width, img_height) // 15  # 15% of the smallest dimension\n",
    "    thickness = max(1, min(img_width, img_height) // 250)  # Keep thickness proportional\n",
    "    color = (0, 255, 0)  # Green\n",
    "\n",
    "    # Ensure coordinates is a list of lists\n",
    "    if isinstance(coordinates, list) and all(isinstance(coord, int) for coord in coordinates):\n",
    "        coordinates = [coordinates]  # Convert single coordinate to a list of lists\n",
    "\n",
    "    for coord in coordinates:\n",
    "        if isinstance(coord, list) and len(coord) == 2:\n",
    "            x, y = coord\n",
    "\n",
    "            # Ensure the bounding box is within image boundaries\n",
    "            top_left_x = max(0, x - box_size // 2)\n",
    "            top_left_y = max(0, y - box_size // 2)\n",
    "            bottom_right_x = min(img_width, x + box_size // 2)\n",
    "            bottom_right_y = min(img_height, y + box_size // 2)\n",
    "\n",
    "            # Draw the rectangle\n",
    "            cv2.rectangle(image, (top_left_x, top_left_y), (bottom_right_x, bottom_right_y), color, thickness)\n",
    "\n",
    "    return image\n",
    "\n",
    "def log_to_file(message):\n",
    "    # Append the message to instructions.txt\n",
    "    with open('gpt4o_instructions.txt', 'a') as file:\n",
    "        file.write(message + '\\n')\n",
    "\n",
    "def process_ui_screens(query_folder, query):\n",
    "    \"\"\"\n",
    "    Process UI screen images with efficient step generation\n",
    "    \"\"\"\n",
    "    # Create output folders\n",
    "    screenshots_path = f'./ui-screens/{query_folder}'\n",
    "    steps_output_folder = f'./gpt4-_steps/{query_folder}'\n",
    "    os.makedirs(steps_output_folder, exist_ok=True)\n",
    "\n",
    "    # Get all files starting with 'frame_'\n",
    "    frame_files = [f for f in os.listdir(screenshots_path) if f.startswith('frame_')]\n",
    "    frame_files.sort(key=lambda f: int(f.split('_')[1].split('.')[0]))\n",
    "\n",
    "    # Track processed steps and previous responses\n",
    "    previous_thoughts = []\n",
    "    previous_actions = []\n",
    "    step_number = 1\n",
    "    previous_response_id = None\n",
    "    complete_found = False\n",
    "\n",
    "    for frame_file in frame_files:\n",
    "        # Skip first two frames if they are intro screens\n",
    "        frame_index = int(frame_file.split('_')[1].split('.')[0])\n",
    "        if frame_index < 2:\n",
    "            message = f\"Skipping {frame_file} as it appears to be an intro screen\"\n",
    "            print(message)\n",
    "            # log_to_file(message)\n",
    "            continue\n",
    "\n",
    "        # If we already found a COMPLETE action, break out of the loop\n",
    "        if complete_found:\n",
    "            break\n",
    "\n",
    "        # Prepare the image path\n",
    "        img_path = os.path.join(screenshots_path, frame_file)\n",
    "        \n",
    "        try:\n",
    "            # Make API call to GPT-4o\n",
    "            response = client.responses.create(\n",
    "                model=\"gpt-4o\",\n",
    "                input=[\n",
    "                    {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                    {\"role\": \"user\", \"content\": [\n",
    "                        {\"type\": \"input_text\", \"text\": f\"Task instruction: '{query}, Provide a unique step that hasn't been mentioned before.'\"},\n",
    "                        {\"type\": \"input_image\", \"image_url\": f\"data:image/jpeg;base64,{encode_image(img_path)}\"}\n",
    "                    ]}\n",
    "                ],\n",
    "                previous_response_id=previous_response_id,\n",
    "            )\n",
    "\n",
    "            # Parse the response\n",
    "            output_text = response.output_text\n",
    "\n",
    "            # Update `previous_response_id` for the next turn\n",
    "            previous_response_id = response.id\n",
    "\n",
    "            try:\n",
    "                # Extract and parse JSON\n",
    "                cleaned_json = extract_json(output_text)\n",
    "                response_data = json.loads(cleaned_json)\n",
    "            except json.JSONDecodeError as json_err:\n",
    "                print(f\"JSON Parsing Error: {json_err}\")\n",
    "                print(f\"Problematic JSON: {output_text}\")\n",
    "                continue\n",
    "\n",
    "            # Skip if action is SKIP\n",
    "            if response_data.get('action') == 'SKIP':\n",
    "                message = \"Skipping frame due to no unique step\"\n",
    "                print(message)\n",
    "                # log_to_file(message)\n",
    "                continue\n",
    "\n",
    "            # Extract details\n",
    "            thought = response_data.get('thought', '')\n",
    "            action = response_data.get('action', '')\n",
    "            coordinates = response_data.get('coordinates', [])\n",
    "\n",
    "            # Print step details\n",
    "            message = f\"Step {step_number}:\"\n",
    "            print(message)\n",
    "            log_to_file(message)\n",
    "            message = f\"Frame: {frame_file}\"\n",
    "            print(message)\n",
    "            log_to_file(message)\n",
    "            message = f\"Thought: {thought}\"\n",
    "            print(message)\n",
    "            log_to_file(message)\n",
    "            message = f\"Action: {action}\"\n",
    "            print(message)\n",
    "            log_to_file(message)\n",
    "            message = f\"Coordinates: {coordinates}\\n\"\n",
    "            print(message)\n",
    "            log_to_file(message)\n",
    "\n",
    "            # Create a step-specific folder and copy the frame\n",
    "            step_folder = os.path.join(steps_output_folder, f'step_{step_number}_frame_{frame_index}')\n",
    "            os.makedirs(step_folder, exist_ok=True)\n",
    "\n",
    "            # If coordinates exist, draw bounding boxes\n",
    "            if coordinates:\n",
    "                modified_image = draw_bounding_boxes(img_path, coordinates)\n",
    "                if modified_image is not None:\n",
    "                    output_img_path = os.path.join(step_folder, frame_file)\n",
    "                    cv2.imwrite(output_img_path, modified_image)\n",
    "                else:\n",
    "                    shutil.copy(img_path, os.path.join(step_folder, frame_file))\n",
    "            else:\n",
    "                shutil.copy(img_path, os.path.join(step_folder, frame_file))\n",
    "\n",
    "            # Save step details as JSON\n",
    "            step_details = {\n",
    "                \"step_number\": step_number,\n",
    "                \"frame\": frame_file,\n",
    "                \"thought\": thought,\n",
    "                \"action\": action,\n",
    "                \"coordinates\": coordinates\n",
    "            }\n",
    "            \n",
    "            with open(os.path.join(step_folder, 'step_details.json'), 'w') as f:\n",
    "                json.dump(step_details, f, indent=4)\n",
    "\n",
    "            # Check if action is COMPLETE - if so, set the flag to stop after this iteration\n",
    "            if action == 'COMPLETE':\n",
    "                message = \"Task completed. Exiting step generation process.\"\n",
    "                print(message)\n",
    "                log_to_file(message)\n",
    "                complete_found = True\n",
    "\n",
    "            # Increment step number\n",
    "            step_number += 1\n",
    "\n",
    "            # Track previous thoughts and actions (only if not COMPLETE)\n",
    "            if action != 'COMPLETE':\n",
    "                previous_thoughts.append(thought)\n",
    "                previous_actions.append(action)\n",
    "\n",
    "        except Exception as e:\n",
    "            message = f\"Error processing {frame_file}: {e}\"\n",
    "            print(message)\n",
    "            # log_to_file(message)\n",
    "            # Skip this frame and move to next\n",
    "            continue\n",
    "\n",
    "    print(\"–––––––––– END ––––––––––\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_ui_screens(query_folder, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20768b46-1f97-40cf-85be-7a260b464cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f04e22-f7fd-499c-abdb-08c01931742c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# OS-ATLAS - ACTION MODEL - WITH BOUNDING BOXES\n",
    "\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import torch\n",
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# Clean up GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "# Load the model and processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"OS-Copilot/OS-Atlas-Base-7B\", torch_dtype=\"auto\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"OS-Copilot/OS-Atlas-Base-7B\",\n",
    "    size=None\n",
    ")\n",
    "\n",
    "# Define the system prompt for Technology Support mode for older adults\n",
    "sys_prompt = \"\"\"  \n",
    "You are in Technology Support mode for older adults. Your role is to assist users over 60 with technology issues by providing step-by-step executable actions.  \n",
    "\n",
    "Guidelines:  \n",
    "1. Step-by-Step Instructions:  \n",
    "    - Provide clear, short steps that are easy to follow.  \n",
    "    - Every action must be directly executable without assumptions.  \n",
    "    - Example: Instead of \"Go to settings,\" specify each step to navigate there.\n",
    "    - Use the images for the step generation:\n",
    "        - Exclude or skip over intro, outro and unclear images.\n",
    "        - Do not use images that do not have interactive UI elements.\n",
    "        - Strict guidelines: ALWAYS generate coordinates relative to the image's size and resolution:\n",
    "            - The coordinates need to be within the image bounds.\n",
    "            - The coordinates need to be correct and accurate location for the UI element mentioned in each step (in 'thought').\n",
    "            - The coordinates should be in abosolute pixel values instead of normalized values. Image resolution is: heightxwidth taken from the `image.shape`.\n",
    "\n",
    "2. Strict Action Format:  \n",
    "    - Each step must have:  \n",
    "        - Thought: Explains the reason for the next action.  \n",
    "        - Action: Specifies what to do in a predefined format.\n",
    "\n",
    "4. No Follow-Up Questions:  \n",
    "    - Do not ask for clarification.  \n",
    "    - Use only given screenshots and action history.\n",
    "\n",
    "Action Formats:  \n",
    "1. CLICK: Click on a position. Format: CLICK <point>[x, y]</point>  \n",
    "2. TYPE: Enter text. Format: TYPE [input text]  \n",
    "3. SCROLL: Scroll in a direction. Format: SCROLL [UP/DOWN/LEFT/RIGHT]  \n",
    "4. OPEN_APP: Open an app. Format: OPEN_APP [app_name]  \n",
    "5. PRESS_BACK: Go to the previous screen. Format: PRESS_BACK  \n",
    "6. PRESS_HOME: Return to the home screen. Format: PRESS_HOME  \n",
    "7. ENTER: Press enter. Format: ENTER  \n",
    "8. WAIT: Pause for loading. Format: WAIT  \n",
    "9. COMPLETE: Task finished. Format: COMPLETE  \n",
    "\n",
    "Example Response:  \n",
    "Query: \"How do I take a screenshot on my iPhone?\"\n",
    "\n",
    "Thought: Open the screen you want to capture.\n",
    "Action: OPEN_APP [Gallery]\n",
    "\n",
    "Thought: Press the correct button combination to take a screenshot.\n",
    "Action: PRESS [Power + Volume Up]\n",
    "\n",
    "Thought: The screenshot is captured and saved.\n",
    "Action: COMPLETE  \n",
    "\"\"\"\n",
    "\n",
    "def extract_coordinates(action, image_width, image_height):\n",
    "    \"\"\"\n",
    "    Extracts normalized coordinates from the action and converts them\n",
    "    to absolute pixel values based on the image dimensions.\n",
    "    \"\"\"\n",
    "    # Patterns to try for coordinate extraction\n",
    "    patterns = [\n",
    "        r'<point>\\[\\[(\\d+),(\\d+)\\]\\]</point>',  # Original pattern with XML-like tags\n",
    "        r'\\[\\[(\\d+),(\\d+)\\]\\]',                 # Nested list format\n",
    "        r'<point>\\[(\\d+),(\\d+)\\]</point>',      # Alternative XML-like tag\n",
    "        r'\\[(\\d+),(\\d+)\\]'                      # Simple list format\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, action)\n",
    "        if match:\n",
    "            x, y = map(int, match.groups())\n",
    "            \n",
    "            # Convert normalized coordinates to absolute pixel values\n",
    "            abs_x = int(x * image_width / 1000)  # Convert normalized x to pixels\n",
    "            abs_y = int(y * image_height / 1000)  # Convert normalized y to pixels\n",
    "\n",
    "            print(f\"Extracted coordinates (normalized): ({x}, {y})\")\n",
    "            print(f\"Converted to absolute pixel values: ({abs_x}, {abs_y})\")\n",
    "            return abs_x, abs_y\n",
    "    \n",
    "    # If no coordinates found, print the full action for debugging\n",
    "    print(f\"No coordinates found in action: {action}\")\n",
    "    return None\n",
    "\n",
    "def draw_bounding_box(image, coordinates, step_number, action_type):\n",
    "    \"\"\"\n",
    "    Draws a bounding box dynamically around the UI element based on a calculated region.\n",
    "    \"\"\"\n",
    "    if coordinates is None:\n",
    "        return image\n",
    "    \n",
    "    x, y = coordinates\n",
    "    \n",
    "    # Validate coordinates are within image bounds\n",
    "    height, width = image.shape[:2]\n",
    "    if x < 0 or x >= width or y < 0 or y >= height:\n",
    "        print(f\"Warning: Coordinates ({x}, {y}) are outside image bounds ({width}x{height})\")\n",
    "        return image\n",
    "    \n",
    "    # Create a copy of the image to draw on\n",
    "    img_with_box = image.copy()\n",
    "    \n",
    "    # Define a dynamic bounding box size based on a percentage of image dimensions\n",
    "    box_size_x = int(width * 0.2)  # 30% of image width for bounding box size\n",
    "    box_size_y = int(height * 0.2)  # 30% of image height for bounding box size\n",
    "    \n",
    "    # Apply a margin around the coordinates for better bounding box positioning\n",
    "    margin = 0.05  # 10% margin to expand the bounding box\n",
    "    expanded_box_size_x = int(box_size_x * (1 + margin))\n",
    "    expanded_box_size_y = int(box_size_y * (1 + margin))\n",
    "    \n",
    "    # Calculate box coordinates with expanded size\n",
    "    top_left = (\n",
    "        max(0, x - expanded_box_size_x // 2), \n",
    "        max(0, y - expanded_box_size_y // 2)\n",
    "    )\n",
    "    bottom_right = (\n",
    "        min(width, x + expanded_box_size_x // 2), \n",
    "        min(height, y + expanded_box_size_y // 2)\n",
    "    )\n",
    "    \n",
    "    # Choose color based on action type\n",
    "    if 'CLICK' in action_type.upper():\n",
    "        color = (0, 255, 0)  # Green for CLICK\n",
    "    elif 'TYPE' in action_type.upper():\n",
    "        color = (255, 0, 0)  # Blue for TYPE\n",
    "    elif 'SCROLL' in action_type.upper():\n",
    "        color = (255, 165, 0)  # Orange for SCROLL\n",
    "    else:\n",
    "        color = (0, 0, 255)  # Red for other actions\n",
    "    \n",
    "    # Draw the rectangle (bounding box)\n",
    "    cv2.rectangle(img_with_box, top_left, bottom_right, color, thickness=2)\n",
    "    \n",
    "    # Add step number text with better text placement\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = min(width, height) / 500  # Dynamic font scaling\n",
    "    font_thickness = 1\n",
    "    \n",
    "    text = f\"Step {step_number}\"\n",
    "    text_size = cv2.getTextSize(text, font, font_scale, font_thickness)[0]\n",
    "    text_pos = (\n",
    "        max(0, top_left[0]), \n",
    "        max(text_size[1] + 10, top_left[1] - 10)\n",
    "    )\n",
    "    \n",
    "    # Add text with a semi-transparent background for better readability\n",
    "    overlay = img_with_box.copy()\n",
    "    cv2.rectangle(\n",
    "        overlay, \n",
    "        (text_pos[0], text_pos[1] - text_size[1] - 10),\n",
    "        (text_pos[0] + text_size[0] + 10, text_pos[1] + 10), \n",
    "        (255, 255, 255), \n",
    "        -1\n",
    "    )\n",
    "    cv2.addWeighted(overlay, 0.5, img_with_box, 0.5, 0, img_with_box)\n",
    "    \n",
    "    # Draw text\n",
    "    cv2.putText(\n",
    "        img_with_box,\n",
    "        text,\n",
    "        text_pos,\n",
    "        font,\n",
    "        font_scale,\n",
    "        color,\n",
    "        font_thickness\n",
    "    )\n",
    "    \n",
    "    return img_with_box\n",
    "\n",
    "\n",
    "def log_to_file(message):\n",
    "    # Append the message to instructions.txt\n",
    "    with open('os_atlas_base_instructions.txt', 'a') as file:\n",
    "        file.write(message + '\\n')\n",
    "\n",
    "\n",
    "# Initialize action history\n",
    "action_history = []\n",
    "\n",
    "# Define the query folder\n",
    "query_folder = \"How_to_restore_the_flashlight_icon_in_the_Control_Center_on_an_iPhone_\"\n",
    "# Define the task instruction\n",
    "query = \"How to restore the flashlight icon in the Control Center on an iPhone?\"\n",
    "\n",
    "# Create output folder for boxed images\n",
    "output_folder = f'./os_atlas_base_steps/{query_folder}'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get all files starting with 'frame_'\n",
    "frame_files = [f for f in os.listdir(f'./ui-screens/{query_folder}') if f.startswith('frame_')]\n",
    "\n",
    "# Sort the files numerically (by extracting the number from the filename)\n",
    "frame_files.sort(key=lambda f: int(f.split('_')[1].split('.')[0]))  # Split based on 'frame_' and take the number\n",
    "\n",
    "step_number = 1\n",
    "\n",
    "# Loop through the frames and process each one\n",
    "for frame_file in frame_files:\n",
    "    # Print the current frame being processed\n",
    "    print(f\"Processing frame: {frame_file}\")\n",
    "\n",
    "    # Prepare the messages for the current image, using the image path\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": sys_prompt},\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": f'./ui-screens/{query_folder}/{frame_file}',\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": f\"Task instruction: '{query}'\\nHistory: {action_history or 'null'}\"}\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    # Generate output for the current frame\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    # Post-process the output\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "    print(f\"OT: {output_text}\")\n",
    "\n",
    "    # Initialize variables\n",
    "    last_step = None\n",
    "    \n",
    "    # Extract thought and action\n",
    "    thought, action = None, None\n",
    "    capturing_thought, capturing_action = False, False  # Flags to track multi-line capturing\n",
    "    temp_thought, temp_action = [], []  # Lists to accumulate multi-line text\n",
    "    \n",
    "    for line in output_text[0].splitlines():\n",
    "        line = line.strip()\n",
    "    \n",
    "        if line.lower().startswith(\"thought:\"):\n",
    "            capturing_thought = True\n",
    "            capturing_action = False  # Stop capturing action if we encounter a new thought\n",
    "            continue  # Move to next line to capture the actual thought\n",
    "    \n",
    "        elif line.lower().startswith(\"actions:\"):\n",
    "            capturing_thought = False\n",
    "            capturing_action = True  # Start capturing action\n",
    "            continue  # Move to next line to capture the actual action\n",
    "    \n",
    "        # Capture multi-line thoughts and actions\n",
    "        if capturing_thought:\n",
    "            temp_thought.append(line)\n",
    "        elif capturing_action:\n",
    "            temp_action.append(line.replace(\"<|im_end|>\", \"\").strip())\n",
    "    \n",
    "    # Join lines to reconstruct the full thought and action\n",
    "    thought = \" \".join(temp_thought).strip() if temp_thought else None\n",
    "    action = \" \".join(temp_action).strip() if temp_action else None\n",
    "    \n",
    "    # Ensure both thought and action exist and check uniqueness\n",
    "    if thought and action:\n",
    "        current_step = (thought, action)\n",
    "    \n",
    "        # Normalize the action for comparison\n",
    "        normalized_action = action.strip().lower()\n",
    "    \n",
    "        # Check if the action has already been performed (in action history)\n",
    "        if normalized_action not in [a.strip().lower() for a in action_history]:\n",
    "            message = f\"Step {step_number}:\"\n",
    "            print(message)\n",
    "            log_to_file(message)\n",
    "            message = f\"Thought: {thought}\"\n",
    "            print(message)\n",
    "            log_to_file(message)\n",
    "            message = f\"Action: {action}\"\n",
    "            print(message)\n",
    "            log_to_file(message)\n",
    "            \n",
    "            # Read the image for bounding box\n",
    "            img_path = f'./ui-screens/{query_folder}/{frame_file}'\n",
    "            img = cv2.imread(img_path)\n",
    "\n",
    "            img_height, img_width, _ = img.shape\n",
    "            \n",
    "            # Check if the action has coordinates\n",
    "            coordinates = extract_coordinates(action, img_width, img_height)\n",
    "\n",
    "            # Create a step-specific folder and copy the frame\n",
    "            step_folder = os.path.join(output_folder, f'step_{step_number}')\n",
    "            os.makedirs(step_folder, exist_ok=True)\n",
    "            \n",
    "            # Draw bounding box for actions with coordinates\n",
    "            if coordinates:\n",
    "                img_with_box = draw_bounding_box(img, coordinates, step_number, action)\n",
    "                # Save the image with bounding box\n",
    "                output_img_path = os.path.join(step_folder, frame_file)\n",
    "                cv2.imwrite(output_img_path, img_with_box)\n",
    "            else:\n",
    "                # If no coordinates, just copy the original image\n",
    "                output_img_path = os.path.join(step_folder, frame_file)\n",
    "                cv2.imwrite(output_img_path, img)\n",
    "\n",
    "            # Save step details as JSON\n",
    "            step_details = {\n",
    "                \"step_number\": step_number,\n",
    "                \"frame\": frame_file,\n",
    "                \"thought\": thought,\n",
    "                \"action\": action,\n",
    "                \"coordinates\": coordinates\n",
    "            }\n",
    "\n",
    "            with open(os.path.join(step_folder, 'step_details.json'), 'w') as f:\n",
    "                json.dump(step_details, f, indent=4)\n",
    "            \n",
    "            action_history.append(action)  # Add only action to the history\n",
    "            last_step = current_step  # Update last step\n",
    "            \n",
    "            print(f\"Action History: {action_history}\\n\")\n",
    "            step_number = step_number + 1\n",
    "    else:\n",
    "        print(\"Thought or action missing for this step.\")\n",
    "        \n",
    "        # Copy the original image to the output folder\n",
    "        img_path = f'./ui-screens/{query_folder}/{frame_file}'\n",
    "        img = cv2.imread(img_path)\n",
    "        output_path = os.path.join(output_folder, frame_file)\n",
    "        cv2.imwrite(output_path, img)\n",
    "\n",
    "        step_number = step_number + 1\n",
    "\n",
    "print(\"–––––– END ––––––\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
